{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import PyPDF2\n",
    "from PyPDF2 import PdfReader\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "import os\n",
    "import codecs\n",
    "from sklearn import feature_extraction\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import silhouette_score\n",
    "from nltk.translate.bleu_score import sentence_bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "stemming = PorterStemmer()\n",
    "nltk.download('stopwords')\n",
    "stopwords = nltk.corpus.stopwords.words(\"english\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Open the PDF file in read-binary mode\n",
    "def read_text(file_name):\n",
    "    with open(file_name, 'rb') as file:\n",
    "        # Create a PDF reader object\n",
    "        reader = PdfReader(file)\n",
    "        info = reader.metadata\n",
    "        all_text=\"\"\n",
    "        # Get the number of pages in the PDF file\n",
    "        num_pages = len(reader.pages)\n",
    "\n",
    "        # Loop through all the pages and extract the text\n",
    "        for i in range(num_pages):\n",
    "            # Get the page object\n",
    "            page =  reader.pages[i]\n",
    "\n",
    "            # Extract the text from the page\n",
    "            text= page.extract_text()\n",
    "            all_text+=text\n",
    "            # Print the text\n",
    "    return str(all_text),info\n",
    "    x,y = read_text(\"files/1-s2.0-S0090429518305971-main.pdf\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = read_text(\"files/1-s2.0-S0090429518305971-main.pdf\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'/AuthoritativeDomain[1]': 'sciencedirect.com', '/Keywords': '', '/Title': 'Predictive Analytics and Modeling Employing Machine Learning Technology: The Next Step in Data Sharing, Analysis, and Individualized Counseling Explored With a Large, Prospective Prenatal Hydronephrosis Database', '/robots': 'noindex', '/Producer': 'Acrobat Distiller 10.1.13 (Windows)', '/CrossmarkDomainExclusive': 'true', '/ModDate': \"D:20181225134835+05'30'\", '/CrossMarkDomains[1]': 'sciencedirect.com', '/Author': 'Armando J. Lorenzo', '/CrossmarkMajorVersionDate': '2010-04-23', '/AuthoritativeDomain[2]': 'elsevier.com', '/Subject': 'Urology, 123 (2018) 204-209. doi:10.1016/j.urology.2018.05.041', '/ElsevierWebPDFSpecifications': '6.5', '/CreationDate': \"D:20181219210605+05'30'\", '/doi': '10.1016/j.urology.2018.05.041', '/CrossMarkDomains[2]': 'elsevier.com', '/Creator': 'Elsevier'}\n"
     ]
    }
   ],
   "source": [
    "print(y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{'/AuthoritativeDomain[1]': 'sciencedirect.com', '/Keywords': '', '/Title': 'Predictive Analytics and Modeling Employing Machine Learning Technology: The Next Step in Data Sharing, Analysis, and Individualized Counseling Explored With a Large, Prospective Prenatal Hydronephrosis Database', '/robots': 'noindex', '/Producer': 'Acrobat Distiller 10.1.13 (Windows)', '/CrossmarkDomainExclusive': 'true', '/ModDate': \"D:20181225134835+05'30'\", '/CrossMarkDomains[1]': 'sciencedirect.com', '/Author': 'Armando J. Lorenzo', '/CrossmarkMajorVersionDate': '2010-04-23', '/AuthoritativeDomain[2]': 'elsevier.com', '/Subject': 'Urology, 123 (2018) 204-209. doi:10.1016/j.urology.2018.05.041', '/ElsevierWebPDFSpecifications': '6.5', '/CreationDate': \"D:20181219210605+05'30'\", '/doi': '10.1016/j.urology.2018.05.041', '/CrossMarkDomains[2]': 'elsevier.com', '/Creator': 'Elsevier'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "stops = set(stopwords)                  \n",
    "\n",
    "def remove_stops(text):\n",
    "    meaningful_words_new=[]\n",
    "    i=0\n",
    "    meaningful_words = [w for w in text if (not w in stops)]\n",
    "    for i in range(len(meaningful_words)-1):\n",
    "        if len(meaningful_words[i])>1:\n",
    "            meaningful_words_new.append(meaningful_words[i])\n",
    "    return (meaningful_words_new)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_stem(text):\n",
    "    stops = set(stopwords) \n",
    "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "    tokens = [word.lower() for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent) if word.isalpha()]\n",
    "    tokens=[w for w in tokens if (not w in stops)&(len(w)>1) ]\n",
    "    filtered_tokens = []\n",
    "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    stems = [stemming.stem(t) for t in filtered_tokens]\n",
    "    \n",
    "    return stems\n",
    "def tokenize_only(text):\n",
    "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "    tokens = [word.lower() for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)if word.isalpha()]\n",
    "    tokens=[w for w in tokens if (not w in stops)&(len(w)>1) ]\n",
    "    filtered_tokens = []\n",
    "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    \n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1-s2.0-S0090429518305971-main.pdf',\n",
       " '1-s2.0-S0302283818307401-main.pdf',\n",
       " '1-s2.0-S258893112300038X-main.pdf',\n",
       " '10.1007@s00345-019-03000-5.pdf',\n",
       " 'abbod2007.pdf',\n",
       " 'AI and reproductive.pdf',\n",
       " 'Cancer Science - 2022 - Iwamura - Machine learning diagnosis by immunoglobulin N‐glycan signatures for precision diagnosis.pdf',\n",
       " 'cci.18.00080.pdf',\n",
       " 'doyle2021.pdf',\n",
       " 'HBP.pdf',\n",
       " 'HBP².pdf',\n",
       " 'icu-61-239.pdf',\n",
       " 'reproductive AI.pdf',\n",
       " 's12911-021-01585-9.pdf',\n",
       " 's41585-020-0312-1.pdf',\n",
       " 'teichmann2007.pdf',\n",
       " 'tju-46-supplement1-s27.pdf',\n",
       " 'urolithiasis and AI.pdf',\n",
       " 'wollin1998 (1).pdf',\n",
       " 'wollin1998.pdf']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = [file for file in os.listdir(\"files\")]\n",
    "file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic = {}\n",
    "dic_tokenize ={}\n",
    "infos={}\n",
    "for i in range(len(file)):\n",
    "    dic[file[i]],info = read_text(\"files/\"+file[i])\n",
    "    infos[i] = info\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Armando J. Lorenzo'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "infos[0].author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "author = []\n",
    "title = []\n",
    "subject = []\n",
    "Id = []\n",
    "creator = []\n",
    "for i in range(len(infos)):\n",
    "    author.append(infos[i].author)\n",
    "    title.append(infos[i].title)\n",
    "    subject.append(infos[i].subject)\n",
    "    creator.append(infos[i].creator)\n",
    "    Id.append(i)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from __future__ import print_function\n",
    "from PyPDF2 import PdfReader\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "import os\n",
    "import codecs\n",
    "from sklearn import feature_extraction\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import silhouette_score\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from typing import List, Dict\n",
    "\n",
    "\n",
    "class PDFReader():\n",
    "\n",
    "    global pdf_dictionary \n",
    "\n",
    "    def __init__(self,directory :str) -> None:\n",
    "        self.directory = directory\n",
    "        self.pdfs_names = [pdf for pdf in os.listdir(directory)]\n",
    "        self.pdf_dictionary = {}\n",
    "        self.infos = {}\n",
    "    \n",
    "    # Open the PDF file in read-binary mode\n",
    "    def read_text(self) -> str:\n",
    "        \n",
    "        \n",
    "        for j in range(len(self.pdfs_names)):\n",
    "            with open(self.directory + \"/\" +self.pdfs_names[j], 'rb') as file:\n",
    "                # Create a PDF reader object\n",
    "                reader = PdfReader(file)\n",
    "                self.infos[j] = reader.metadata\n",
    "                all_text=\"\"\n",
    "                # Get the number of pages in the PDF file\n",
    "                num_pages = len(reader.pages)\n",
    "\n",
    "                # Loop through all the pages and extract the text\n",
    "                for i in range(num_pages):\n",
    "                    # Get the page object\n",
    "                    page =  reader.pages[i]\n",
    "                    # Extract the text from the page\n",
    "                    text= page.extract_text()\n",
    "                    all_text+=text\n",
    "                    # Print the text\n",
    "                self.pdf_dictionary[self.pdfs_names[j]]= all_text\n",
    "        return self.pdf_dictionary,self.infos\n",
    "        \n",
    "\n",
    "class TextPrecessor():\n",
    "    \n",
    "    def __init__(self) -> None:\n",
    "        self.stemming = PorterStemmer()\n",
    "        self.stopwords = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "        self.data = pd.DataFrame({\n",
    "            \"text\": [],\n",
    "            \"titre\": [],\n",
    "            \"tokenize_text\":[],\n",
    "            \"Author\" : [],\n",
    "            \"subject\" : [],\n",
    "            \"Id\" : [],\n",
    "            \"Creator\" : [],\n",
    "            \"title\" : [],\n",
    "            })\n",
    "        self.preprocess_data()\n",
    "\n",
    "\n",
    "    def tokenize_and_stem(self,text) -> list:\n",
    "        # Call the read_text method to get the dictionary of PDF content\n",
    "        tokens = [word.lower() for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent) if word.isalpha()]\n",
    "        tokens=[w for w in tokens if (not w in self.stopwords)&(len(w)>1) ]\n",
    "        filtered_tokens = []\n",
    "            # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "        for token in tokens:\n",
    "            if re.search('[a-zA-Z]', token):\n",
    "                filtered_tokens.append(token)\n",
    "        stems = [self.stemming.stem(t) for t in filtered_tokens]\n",
    "        return stems\n",
    "\n",
    "    def preprocess_data(self):\n",
    "        # Call the tokenize_and_stem method to tokenize and stem the PDF content\n",
    "        pdf_reader = PDFReader(\"files\")\n",
    "        pdf_dictionary,infos = pdf_reader.read_text()\n",
    "        for i, pdf_name in enumerate(pdf_dictionary.keys()):\n",
    "            \n",
    "            # Append the data to the DataFrame\n",
    "            self.data = self.data.append({\n",
    "                \"text\": pdf_dictionary[pdf_name],\n",
    "                \"titre\": pdf_name,\n",
    "                \"tokenize_text\": self.tokenize_and_stem(pdf_dictionary[pdf_name]),\n",
    "                \"Author\" : infos[i].author ,\n",
    "                \"subject\" : infos[i].subject,\n",
    "                \"Id\" : i+1,\n",
    "                \"Creator\" : infos[i].creator,\n",
    "                \"title\" : infos[i].title,\n",
    "            }, ignore_index=True)\n",
    "        self.data.fillna(\"Unkown\").replace(\"\",\"Unknown\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_116292\\647506.py:93: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  self.data = self.data.append({\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_116292\\647506.py:93: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  self.data = self.data.append({\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_116292\\647506.py:93: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  self.data = self.data.append({\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_116292\\647506.py:93: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  self.data = self.data.append({\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_116292\\647506.py:93: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  self.data = self.data.append({\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_116292\\647506.py:93: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  self.data = self.data.append({\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_116292\\647506.py:93: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  self.data = self.data.append({\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_116292\\647506.py:93: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  self.data = self.data.append({\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_116292\\647506.py:93: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  self.data = self.data.append({\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_116292\\647506.py:93: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  self.data = self.data.append({\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_116292\\647506.py:93: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  self.data = self.data.append({\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_116292\\647506.py:93: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  self.data = self.data.append({\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_116292\\647506.py:93: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  self.data = self.data.append({\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_116292\\647506.py:93: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  self.data = self.data.append({\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_116292\\647506.py:93: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  self.data = self.data.append({\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_116292\\647506.py:93: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  self.data = self.data.append({\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_116292\\647506.py:93: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  self.data = self.data.append({\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_116292\\647506.py:93: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  self.data = self.data.append({\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_116292\\647506.py:93: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  self.data = self.data.append({\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_116292\\647506.py:93: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  self.data = self.data.append({\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>titre</th>\n",
       "      <th>tokenize_text</th>\n",
       "      <th>Author</th>\n",
       "      <th>subject</th>\n",
       "      <th>Id</th>\n",
       "      <th>Creator</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Pediatric Urology\\nPredictive Analytics and Mo...</td>\n",
       "      <td>1-s2.0-S0090429518305971-main.pdf</td>\n",
       "      <td>[pediatr, urolog, predict, analyt, model, empl...</td>\n",
       "      <td>Armando J. Lorenzo</td>\n",
       "      <td>Urology, 123 (2018) 204-209. doi:10.1016/j.uro...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Elsevier</td>\n",
       "      <td>Predictive Analytics and Modeling Employing Ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Platinum Priority –Prostate Cancer –Editor ’s ...</td>\n",
       "      <td>1-s2.0-S0302283818307401-main.pdf</td>\n",
       "      <td>[platinum, prioriti, cancer, choic, editori, p...</td>\n",
       "      <td>Gregory B. Auffenberg</td>\n",
       "      <td>European Urology, 75 (2019) 901-907. doi:10.10...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Elsevier</td>\n",
       "      <td>askMUSIC: Leveraging a Clinical Registry to De...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Development and External Validation of a Machi...</td>\n",
       "      <td>1-s2.0-S258893112300038X-main.pdf</td>\n",
       "      <td>[develop, extern, valid, machin, learn, model,...</td>\n",
       "      <td>Ali Sabbagh</td>\n",
       "      <td>European Urology Oncology, Corrected proof. do...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Elsevier</td>\n",
       "      <td>Development and External Validation of a Machi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Vol.:(0123456789)1 3World Journal of Urology \\...</td>\n",
       "      <td>10.1007@s00345-019-03000-5.pdf</td>\n",
       "      <td>[vol, journal, urolog, http, topic, paper, cur...</td>\n",
       "      <td>Rodrigo Suarez-Ibarrola</td>\n",
       "      <td>World Journal of Urology, https://doi.org/10.1...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Springer</td>\n",
       "      <td>Current and future applications of machine and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Application of Artiﬁcial Intelligence\\nto the ...</td>\n",
       "      <td>abbod2007.pdf</td>\n",
       "      <td>[applic, artiﬁci, intellig, manag, urolog, can...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>5.0</td>\n",
       "      <td>Elsevier</td>\n",
       "      <td>doi:10.1016/j.juro.2007.05.122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>MEN'S HEALTH (A DABAJA, SECTION EDITOR)\\nArtif...</td>\n",
       "      <td>AI and reproductive.pdf</td>\n",
       "      <td>[men, health, dabaja, section, editor, artific...</td>\n",
       "      <td>Kevin Y. Chu; Daniel E. Nassau; Himanshu Arora...</td>\n",
       "      <td>Curr Urol Rep, doi:10.1007/s11934-019-0914-4</td>\n",
       "      <td>6.0</td>\n",
       "      <td>Arbortext Advanced Print Publisher 9.1.440/W U...</td>\n",
       "      <td>Artificial Intelligence in Reproductive Urology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2434  |     Cancer Science. 2022;113:2434–2445...</td>\n",
       "      <td>Cancer Science - 2022 - Iwamura - Machine lear...</td>\n",
       "      <td>[cancer, scienc, march, revis, april, accept, ...</td>\n",
       "      <td>None</td>\n",
       "      <td>Cancer Science 2022.113:2434-2445</td>\n",
       "      <td>7.0</td>\n",
       "      <td>Adobe InDesign 15.0 (Windows)</td>\n",
       "      <td>Machine learning diagnosis by immunoglobulin N...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>© 2018 by American Society of Clinical Oncolog...</td>\n",
       "      <td>cci.18.00080.pdf</td>\n",
       "      <td>[american, societi, clinic, oncolog, clinic, c...</td>\n",
       "      <td>Sami-Ramzi Leyh-Bannurah, Zhe Tian, Pierre I. ...</td>\n",
       "      <td>JCO Clinical Cancer Informatics 2018.:1-9</td>\n",
       "      <td>8.0</td>\n",
       "      <td>Adobe InDesign CC 13.0 (Windows)</td>\n",
       "      <td>Deep Learning for Natural Language Processing ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Vol.:(0123456789)1 3World Journal of Urology \\...</td>\n",
       "      <td>doyle2021.pdf</td>\n",
       "      <td>[vol, journal, urolog, http, topic, paper, mac...</td>\n",
       "      <td>Patrick W. Doyle</td>\n",
       "      <td>World Journal of Urology, https://doi.org/10.1...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>Springer</td>\n",
       "      <td>Machine learning applications to enhance patie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>c  o  m  p  u  t  e  r  m  e  t  h  o  d  s  a...</td>\n",
       "      <td>HBP.pdf</td>\n",
       "      <td>[jo, ur, nal, ho, ag, hybrid, base, intellig, ...</td>\n",
       "      <td>Abolfazl Doostparast Torshizi</td>\n",
       "      <td>Computer Methods and Programs in Biomedicine, ...</td>\n",
       "      <td>10.0</td>\n",
       "      <td>Elsevier</td>\n",
       "      <td>A hybrid fuzzy-ontology based intelligent syst...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>PREDICTION OF BLADDER OUTLET OBSTRUCTION IN ME...</td>\n",
       "      <td>HBP².pdf</td>\n",
       "      <td>[predict, bladder, outlet, obstruct, men, lowe...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>11.0</td>\n",
       "      <td>Elsevier</td>\n",
       "      <td>PII: S0022-5347(05)68042-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>239Editorial\\nⓒ The Korean Urological Associat...</td>\n",
       "      <td>icu-61-239.pdf</td>\n",
       "      <td>[korean, urolog, associ, clin, urol, http, pis...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>12.0</td>\n",
       "      <td>Adobe InDesign 14.0 (Windows)</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>TYPEOriginal Research\\nPUBLISHED /one.tnum/thr...</td>\n",
       "      <td>reproductive AI.pdf</td>\n",
       "      <td>[typeorigin, research, publish, septemb, opena...</td>\n",
       "      <td>Yunfeng Guan, Aijun Zhang and Bufang Xu</td>\n",
       "      <td>Introduction: Semen quality has decreased grad...</td>\n",
       "      <td>13.0</td>\n",
       "      <td>LaTeX with hyperref package + hypdvips</td>\n",
       "      <td>Preliminary prediction of semen quality based ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Salem  et al. BMC Med Inform Decis Mak        ...</td>\n",
       "      <td>s12911-021-01585-9.pdf</td>\n",
       "      <td>[salem, et, al, bmc, med, inform, deci, mak, h...</td>\n",
       "      <td>Hesham Salem</td>\n",
       "      <td>BMC Medical Informatics and Decision Making, h...</td>\n",
       "      <td>14.0</td>\n",
       "      <td>Adobe InDesign 15.0 (Windows)</td>\n",
       "      <td>A systematic review of the applications of Exp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>COmmenT\\nNature revIeWS | UrOlOgyThe situation...</td>\n",
       "      <td>s41585-020-0312-1.pdf</td>\n",
       "      <td>[comment, natur, review, urologyth, situat, lo...</td>\n",
       "      <td>Richard Naspro</td>\n",
       "      <td>Nature Reviews Urology, doi:10.1038/s41585-020...</td>\n",
       "      <td>15.0</td>\n",
       "      <td>Springer</td>\n",
       "      <td>Urology in the time of corona</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>World J Urol (2007) 25:221–225 \\nDOI 10.1007/s...</td>\n",
       "      <td>teichmann2007.pdf</td>\n",
       "      <td>[world, urol, doi, paper, technic, aspect, las...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>16.0</td>\n",
       "      <td>FrameMaker 7.1</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Invited Review S27\\nENDOUROLOGYTurk J Urol 202...</td>\n",
       "      <td>tju-46-supplement1-s27.pdf</td>\n",
       "      <td>[invit, review, endourologyturk, urol, supp, d...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>17.0</td>\n",
       "      <td>Adobe InDesign 15.1 (Macintosh)</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>A novel approach for accurate prediction of\\ns...</td>\n",
       "      <td>urolithiasis and AI.pdf</td>\n",
       "      <td>[novel, approach, accur, predict, spontan, pas...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>18.0</td>\n",
       "      <td>3B2 Total Publishing System 8.07f/W</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>ournalC uicCLaLslC elM&amp;n S&amp;gLsLa&amp; yVrnm&amp;n1\\n,u...</td>\n",
       "      <td>wollin1998 (1).pdf</td>\n",
       "      <td>[ournalc, uicclalslc, elm, glsla, yvrnm, ucrn,...</td>\n",
       "      <td>TIM A. WOLLIN and JOHN D. DENSTEDT</td>\n",
       "      <td>Journal of Clinical Laser Medicine &amp; Surgery 1...</td>\n",
       "      <td>19.0</td>\n",
       "      <td>Apex CoVantage LLC.</td>\n",
       "      <td>The Holmium Laser in Urology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>ournalC uicCLaLslC elM&amp;n S&amp;gLsLa&amp; yVrnm&amp;n1\\n,u...</td>\n",
       "      <td>wollin1998.pdf</td>\n",
       "      <td>[ournalc, uicclalslc, elm, glsla, yvrnm, ucrn,...</td>\n",
       "      <td>TIM A. WOLLIN and JOHN D. DENSTEDT</td>\n",
       "      <td>Journal of Clinical Laser Medicine &amp; Surgery 1...</td>\n",
       "      <td>20.0</td>\n",
       "      <td>Apex CoVantage LLC.</td>\n",
       "      <td>The Holmium Laser in Urology</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text  \\\n",
       "0   Pediatric Urology\\nPredictive Analytics and Mo...   \n",
       "1   Platinum Priority –Prostate Cancer –Editor ’s ...   \n",
       "2   Development and External Validation of a Machi...   \n",
       "3   Vol.:(0123456789)1 3World Journal of Urology \\...   \n",
       "4   Application of Artiﬁcial Intelligence\\nto the ...   \n",
       "5   MEN'S HEALTH (A DABAJA, SECTION EDITOR)\\nArtif...   \n",
       "6   2434  |     Cancer Science. 2022;113:2434–2445...   \n",
       "7   © 2018 by American Society of Clinical Oncolog...   \n",
       "8   Vol.:(0123456789)1 3World Journal of Urology \\...   \n",
       "9   c  o  m  p  u  t  e  r  m  e  t  h  o  d  s  a...   \n",
       "10  PREDICTION OF BLADDER OUTLET OBSTRUCTION IN ME...   \n",
       "11  239Editorial\\nⓒ The Korean Urological Associat...   \n",
       "12  TYPEOriginal Research\\nPUBLISHED /one.tnum/thr...   \n",
       "13  Salem  et al. BMC Med Inform Decis Mak        ...   \n",
       "14  COmmenT\\nNature revIeWS | UrOlOgyThe situation...   \n",
       "15  World J Urol (2007) 25:221–225 \\nDOI 10.1007/s...   \n",
       "16  Invited Review S27\\nENDOUROLOGYTurk J Urol 202...   \n",
       "17  A novel approach for accurate prediction of\\ns...   \n",
       "18  ournalC uicCLaLslC elM&n S&gLsLa& yVrnm&n1\\n,u...   \n",
       "19  ournalC uicCLaLslC elM&n S&gLsLa& yVrnm&n1\\n,u...   \n",
       "\n",
       "                                                titre  \\\n",
       "0                   1-s2.0-S0090429518305971-main.pdf   \n",
       "1                   1-s2.0-S0302283818307401-main.pdf   \n",
       "2                   1-s2.0-S258893112300038X-main.pdf   \n",
       "3                      10.1007@s00345-019-03000-5.pdf   \n",
       "4                                       abbod2007.pdf   \n",
       "5                             AI and reproductive.pdf   \n",
       "6   Cancer Science - 2022 - Iwamura - Machine lear...   \n",
       "7                                    cci.18.00080.pdf   \n",
       "8                                       doyle2021.pdf   \n",
       "9                                             HBP.pdf   \n",
       "10                                           HBP².pdf   \n",
       "11                                     icu-61-239.pdf   \n",
       "12                                reproductive AI.pdf   \n",
       "13                             s12911-021-01585-9.pdf   \n",
       "14                              s41585-020-0312-1.pdf   \n",
       "15                                  teichmann2007.pdf   \n",
       "16                         tju-46-supplement1-s27.pdf   \n",
       "17                            urolithiasis and AI.pdf   \n",
       "18                                 wollin1998 (1).pdf   \n",
       "19                                     wollin1998.pdf   \n",
       "\n",
       "                                        tokenize_text  \\\n",
       "0   [pediatr, urolog, predict, analyt, model, empl...   \n",
       "1   [platinum, prioriti, cancer, choic, editori, p...   \n",
       "2   [develop, extern, valid, machin, learn, model,...   \n",
       "3   [vol, journal, urolog, http, topic, paper, cur...   \n",
       "4   [applic, artiﬁci, intellig, manag, urolog, can...   \n",
       "5   [men, health, dabaja, section, editor, artific...   \n",
       "6   [cancer, scienc, march, revis, april, accept, ...   \n",
       "7   [american, societi, clinic, oncolog, clinic, c...   \n",
       "8   [vol, journal, urolog, http, topic, paper, mac...   \n",
       "9   [jo, ur, nal, ho, ag, hybrid, base, intellig, ...   \n",
       "10  [predict, bladder, outlet, obstruct, men, lowe...   \n",
       "11  [korean, urolog, associ, clin, urol, http, pis...   \n",
       "12  [typeorigin, research, publish, septemb, opena...   \n",
       "13  [salem, et, al, bmc, med, inform, deci, mak, h...   \n",
       "14  [comment, natur, review, urologyth, situat, lo...   \n",
       "15  [world, urol, doi, paper, technic, aspect, las...   \n",
       "16  [invit, review, endourologyturk, urol, supp, d...   \n",
       "17  [novel, approach, accur, predict, spontan, pas...   \n",
       "18  [ournalc, uicclalslc, elm, glsla, yvrnm, ucrn,...   \n",
       "19  [ournalc, uicclalslc, elm, glsla, yvrnm, ucrn,...   \n",
       "\n",
       "                                               Author  \\\n",
       "0                                  Armando J. Lorenzo   \n",
       "1                               Gregory B. Auffenberg   \n",
       "2                                         Ali Sabbagh   \n",
       "3                            Rodrigo Suarez-Ibarrola    \n",
       "4                                                       \n",
       "5   Kevin Y. Chu; Daniel E. Nassau; Himanshu Arora...   \n",
       "6                                                None   \n",
       "7   Sami-Ramzi Leyh-Bannurah, Zhe Tian, Pierre I. ...   \n",
       "8                                   Patrick W. Doyle    \n",
       "9                       Abolfazl Doostparast Torshizi   \n",
       "10                                               None   \n",
       "11                                               None   \n",
       "12            Yunfeng Guan, Aijun Zhang and Bufang Xu   \n",
       "13                                      Hesham Salem    \n",
       "14                                     Richard Naspro   \n",
       "15                                               None   \n",
       "16                                               None   \n",
       "17                                               None   \n",
       "18                 TIM A. WOLLIN and JOHN D. DENSTEDT   \n",
       "19                 TIM A. WOLLIN and JOHN D. DENSTEDT   \n",
       "\n",
       "                                              subject    Id  \\\n",
       "0   Urology, 123 (2018) 204-209. doi:10.1016/j.uro...   1.0   \n",
       "1   European Urology, 75 (2019) 901-907. doi:10.10...   2.0   \n",
       "2   European Urology Oncology, Corrected proof. do...   3.0   \n",
       "3   World Journal of Urology, https://doi.org/10.1...   4.0   \n",
       "4                                                       5.0   \n",
       "5        Curr Urol Rep, doi:10.1007/s11934-019-0914-4   6.0   \n",
       "6                   Cancer Science 2022.113:2434-2445   7.0   \n",
       "7           JCO Clinical Cancer Informatics 2018.:1-9   8.0   \n",
       "8   World Journal of Urology, https://doi.org/10.1...   9.0   \n",
       "9   Computer Methods and Programs in Biomedicine, ...  10.0   \n",
       "10                                               None  11.0   \n",
       "11                                               None  12.0   \n",
       "12  Introduction: Semen quality has decreased grad...  13.0   \n",
       "13  BMC Medical Informatics and Decision Making, h...  14.0   \n",
       "14  Nature Reviews Urology, doi:10.1038/s41585-020...  15.0   \n",
       "15                                               None  16.0   \n",
       "16                                               None  17.0   \n",
       "17                                               None  18.0   \n",
       "18  Journal of Clinical Laser Medicine & Surgery 1...  19.0   \n",
       "19  Journal of Clinical Laser Medicine & Surgery 1...  20.0   \n",
       "\n",
       "                                              Creator  \\\n",
       "0                                            Elsevier   \n",
       "1                                            Elsevier   \n",
       "2                                            Elsevier   \n",
       "3                                            Springer   \n",
       "4                                            Elsevier   \n",
       "5   Arbortext Advanced Print Publisher 9.1.440/W U...   \n",
       "6                       Adobe InDesign 15.0 (Windows)   \n",
       "7                    Adobe InDesign CC 13.0 (Windows)   \n",
       "8                                            Springer   \n",
       "9                                            Elsevier   \n",
       "10                                           Elsevier   \n",
       "11                      Adobe InDesign 14.0 (Windows)   \n",
       "12             LaTeX with hyperref package + hypdvips   \n",
       "13                      Adobe InDesign 15.0 (Windows)   \n",
       "14                                           Springer   \n",
       "15                                     FrameMaker 7.1   \n",
       "16                    Adobe InDesign 15.1 (Macintosh)   \n",
       "17                3B2 Total Publishing System 8.07f/W   \n",
       "18                                Apex CoVantage LLC.   \n",
       "19                                Apex CoVantage LLC.   \n",
       "\n",
       "                                                title  \n",
       "0   Predictive Analytics and Modeling Employing Ma...  \n",
       "1   askMUSIC: Leveraging a Clinical Registry to De...  \n",
       "2   Development and External Validation of a Machi...  \n",
       "3   Current and future applications of machine and...  \n",
       "4                      doi:10.1016/j.juro.2007.05.122  \n",
       "5     Artificial Intelligence in Reproductive Urology  \n",
       "6   Machine learning diagnosis by immunoglobulin N...  \n",
       "7   Deep Learning for Natural Language Processing ...  \n",
       "8   Machine learning applications to enhance patie...  \n",
       "9   A hybrid fuzzy-ontology based intelligent syst...  \n",
       "10                         PII: S0022-5347(05)68042-1  \n",
       "11                                               None  \n",
       "12  Preliminary prediction of semen quality based ...  \n",
       "13  A systematic review of the applications of Exp...  \n",
       "14                      Urology in the time of corona  \n",
       "15                                               None  \n",
       "16                                               None  \n",
       "17                                               None  \n",
       "18                       The Holmium Laser in Urology  \n",
       "19                       The Holmium Laser in Urology  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instance = TextPrecessor()\n",
    "instance.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "  \"text\": [item for item in dic.values()],\n",
    "  \"titre\": file,\n",
    "  \"similarity\":[0 for i in range(len(file))],\n",
    "  \"Author\" : author,\n",
    "  \"subject\" : subject,\n",
    "  \"Id\" : Id,\n",
    "  \"Creator\" : creator,\n",
    "  \"title\" : title,\n",
    "}\n",
    "#load data into a DataFrame object:\n",
    "data= pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>titre</th>\n",
       "      <th>similarity</th>\n",
       "      <th>Author</th>\n",
       "      <th>subject</th>\n",
       "      <th>Id</th>\n",
       "      <th>Creator</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Pediatric Urology\\nPredictive Analytics and Mo...</td>\n",
       "      <td>1-s2.0-S0090429518305971-main.pdf</td>\n",
       "      <td>0</td>\n",
       "      <td>Armando J. Lorenzo</td>\n",
       "      <td>Urology, 123 (2018) 204-209. doi:10.1016/j.uro...</td>\n",
       "      <td>0</td>\n",
       "      <td>Elsevier</td>\n",
       "      <td>Predictive Analytics and Modeling Employing Ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Platinum Priority –Prostate Cancer –Editor ’s ...</td>\n",
       "      <td>1-s2.0-S0302283818307401-main.pdf</td>\n",
       "      <td>0</td>\n",
       "      <td>Gregory B. Auffenberg</td>\n",
       "      <td>European Urology, 75 (2019) 901-907. doi:10.10...</td>\n",
       "      <td>1</td>\n",
       "      <td>Elsevier</td>\n",
       "      <td>askMUSIC: Leveraging a Clinical Registry to De...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Development and External Validation of a Machi...</td>\n",
       "      <td>1-s2.0-S258893112300038X-main.pdf</td>\n",
       "      <td>0</td>\n",
       "      <td>Ali Sabbagh</td>\n",
       "      <td>European Urology Oncology, Corrected proof. do...</td>\n",
       "      <td>2</td>\n",
       "      <td>Elsevier</td>\n",
       "      <td>Development and External Validation of a Machi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Vol.:(0123456789)1 3World Journal of Urology \\...</td>\n",
       "      <td>10.1007@s00345-019-03000-5.pdf</td>\n",
       "      <td>0</td>\n",
       "      <td>Rodrigo Suarez-Ibarrola</td>\n",
       "      <td>World Journal of Urology, https://doi.org/10.1...</td>\n",
       "      <td>3</td>\n",
       "      <td>Springer</td>\n",
       "      <td>Current and future applications of machine and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Application of Artiﬁcial Intelligence\\nto the ...</td>\n",
       "      <td>abbod2007.pdf</td>\n",
       "      <td>0</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>4</td>\n",
       "      <td>Elsevier</td>\n",
       "      <td>doi:10.1016/j.juro.2007.05.122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>MEN'S HEALTH (A DABAJA, SECTION EDITOR)\\nArtif...</td>\n",
       "      <td>AI and reproductive.pdf</td>\n",
       "      <td>0</td>\n",
       "      <td>Kevin Y. Chu; Daniel E. Nassau; Himanshu Arora...</td>\n",
       "      <td>Curr Urol Rep, doi:10.1007/s11934-019-0914-4</td>\n",
       "      <td>5</td>\n",
       "      <td>Arbortext Advanced Print Publisher 9.1.440/W U...</td>\n",
       "      <td>Artificial Intelligence in Reproductive Urology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2434  |     Cancer Science. 2022;113:2434–2445...</td>\n",
       "      <td>Cancer Science - 2022 - Iwamura - Machine lear...</td>\n",
       "      <td>0</td>\n",
       "      <td>Unkown</td>\n",
       "      <td>Cancer Science 2022.113:2434-2445</td>\n",
       "      <td>6</td>\n",
       "      <td>Adobe InDesign 15.0 (Windows)</td>\n",
       "      <td>Machine learning diagnosis by immunoglobulin N...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>© 2018 by American Society of Clinical Oncolog...</td>\n",
       "      <td>cci.18.00080.pdf</td>\n",
       "      <td>0</td>\n",
       "      <td>Sami-Ramzi Leyh-Bannurah, Zhe Tian, Pierre I. ...</td>\n",
       "      <td>JCO Clinical Cancer Informatics 2018.:1-9</td>\n",
       "      <td>7</td>\n",
       "      <td>Adobe InDesign CC 13.0 (Windows)</td>\n",
       "      <td>Deep Learning for Natural Language Processing ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Vol.:(0123456789)1 3World Journal of Urology \\...</td>\n",
       "      <td>doyle2021.pdf</td>\n",
       "      <td>0</td>\n",
       "      <td>Patrick W. Doyle</td>\n",
       "      <td>World Journal of Urology, https://doi.org/10.1...</td>\n",
       "      <td>8</td>\n",
       "      <td>Springer</td>\n",
       "      <td>Machine learning applications to enhance patie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>c  o  m  p  u  t  e  r  m  e  t  h  o  d  s  a...</td>\n",
       "      <td>HBP.pdf</td>\n",
       "      <td>0</td>\n",
       "      <td>Abolfazl Doostparast Torshizi</td>\n",
       "      <td>Computer Methods and Programs in Biomedicine, ...</td>\n",
       "      <td>9</td>\n",
       "      <td>Elsevier</td>\n",
       "      <td>A hybrid fuzzy-ontology based intelligent syst...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>PREDICTION OF BLADDER OUTLET OBSTRUCTION IN ME...</td>\n",
       "      <td>HBP².pdf</td>\n",
       "      <td>0</td>\n",
       "      <td>Unkown</td>\n",
       "      <td>Unkown</td>\n",
       "      <td>10</td>\n",
       "      <td>Elsevier</td>\n",
       "      <td>PII: S0022-5347(05)68042-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>239Editorial\\nⓒ The Korean Urological Associat...</td>\n",
       "      <td>icu-61-239.pdf</td>\n",
       "      <td>0</td>\n",
       "      <td>Unkown</td>\n",
       "      <td>Unkown</td>\n",
       "      <td>11</td>\n",
       "      <td>Adobe InDesign 14.0 (Windows)</td>\n",
       "      <td>Unkown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>TYPEOriginal Research\\nPUBLISHED /one.tnum/thr...</td>\n",
       "      <td>reproductive AI.pdf</td>\n",
       "      <td>0</td>\n",
       "      <td>Yunfeng Guan, Aijun Zhang and Bufang Xu</td>\n",
       "      <td>Introduction: Semen quality has decreased grad...</td>\n",
       "      <td>12</td>\n",
       "      <td>LaTeX with hyperref package + hypdvips</td>\n",
       "      <td>Preliminary prediction of semen quality based ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Salem  et al. BMC Med Inform Decis Mak        ...</td>\n",
       "      <td>s12911-021-01585-9.pdf</td>\n",
       "      <td>0</td>\n",
       "      <td>Hesham Salem</td>\n",
       "      <td>BMC Medical Informatics and Decision Making, h...</td>\n",
       "      <td>13</td>\n",
       "      <td>Adobe InDesign 15.0 (Windows)</td>\n",
       "      <td>A systematic review of the applications of Exp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>COmmenT\\nNature revIeWS | UrOlOgyThe situation...</td>\n",
       "      <td>s41585-020-0312-1.pdf</td>\n",
       "      <td>0</td>\n",
       "      <td>Richard Naspro</td>\n",
       "      <td>Nature Reviews Urology, doi:10.1038/s41585-020...</td>\n",
       "      <td>14</td>\n",
       "      <td>Springer</td>\n",
       "      <td>Urology in the time of corona</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>World J Urol (2007) 25:221–225 \\nDOI 10.1007/s...</td>\n",
       "      <td>teichmann2007.pdf</td>\n",
       "      <td>0</td>\n",
       "      <td>Unkown</td>\n",
       "      <td>Unkown</td>\n",
       "      <td>15</td>\n",
       "      <td>FrameMaker 7.1</td>\n",
       "      <td>Unkown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Invited Review S27\\nENDOUROLOGYTurk J Urol 202...</td>\n",
       "      <td>tju-46-supplement1-s27.pdf</td>\n",
       "      <td>0</td>\n",
       "      <td>Unkown</td>\n",
       "      <td>Unkown</td>\n",
       "      <td>16</td>\n",
       "      <td>Adobe InDesign 15.1 (Macintosh)</td>\n",
       "      <td>Unkown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>A novel approach for accurate prediction of\\ns...</td>\n",
       "      <td>urolithiasis and AI.pdf</td>\n",
       "      <td>0</td>\n",
       "      <td>Unkown</td>\n",
       "      <td>Unkown</td>\n",
       "      <td>17</td>\n",
       "      <td>3B2 Total Publishing System 8.07f/W</td>\n",
       "      <td>Unkown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>ournalC uicCLaLslC elM&amp;n S&amp;gLsLa&amp; yVrnm&amp;n1\\n,u...</td>\n",
       "      <td>wollin1998 (1).pdf</td>\n",
       "      <td>0</td>\n",
       "      <td>TIM A. WOLLIN and JOHN D. DENSTEDT</td>\n",
       "      <td>Journal of Clinical Laser Medicine &amp; Surgery 1...</td>\n",
       "      <td>18</td>\n",
       "      <td>Apex CoVantage LLC.</td>\n",
       "      <td>The Holmium Laser in Urology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>ournalC uicCLaLslC elM&amp;n S&amp;gLsLa&amp; yVrnm&amp;n1\\n,u...</td>\n",
       "      <td>wollin1998.pdf</td>\n",
       "      <td>0</td>\n",
       "      <td>TIM A. WOLLIN and JOHN D. DENSTEDT</td>\n",
       "      <td>Journal of Clinical Laser Medicine &amp; Surgery 1...</td>\n",
       "      <td>19</td>\n",
       "      <td>Apex CoVantage LLC.</td>\n",
       "      <td>The Holmium Laser in Urology</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text  \\\n",
       "0   Pediatric Urology\\nPredictive Analytics and Mo...   \n",
       "1   Platinum Priority –Prostate Cancer –Editor ’s ...   \n",
       "2   Development and External Validation of a Machi...   \n",
       "3   Vol.:(0123456789)1 3World Journal of Urology \\...   \n",
       "4   Application of Artiﬁcial Intelligence\\nto the ...   \n",
       "5   MEN'S HEALTH (A DABAJA, SECTION EDITOR)\\nArtif...   \n",
       "6   2434  |     Cancer Science. 2022;113:2434–2445...   \n",
       "7   © 2018 by American Society of Clinical Oncolog...   \n",
       "8   Vol.:(0123456789)1 3World Journal of Urology \\...   \n",
       "9   c  o  m  p  u  t  e  r  m  e  t  h  o  d  s  a...   \n",
       "10  PREDICTION OF BLADDER OUTLET OBSTRUCTION IN ME...   \n",
       "11  239Editorial\\nⓒ The Korean Urological Associat...   \n",
       "12  TYPEOriginal Research\\nPUBLISHED /one.tnum/thr...   \n",
       "13  Salem  et al. BMC Med Inform Decis Mak        ...   \n",
       "14  COmmenT\\nNature revIeWS | UrOlOgyThe situation...   \n",
       "15  World J Urol (2007) 25:221–225 \\nDOI 10.1007/s...   \n",
       "16  Invited Review S27\\nENDOUROLOGYTurk J Urol 202...   \n",
       "17  A novel approach for accurate prediction of\\ns...   \n",
       "18  ournalC uicCLaLslC elM&n S&gLsLa& yVrnm&n1\\n,u...   \n",
       "19  ournalC uicCLaLslC elM&n S&gLsLa& yVrnm&n1\\n,u...   \n",
       "\n",
       "                                                titre  similarity  \\\n",
       "0                   1-s2.0-S0090429518305971-main.pdf           0   \n",
       "1                   1-s2.0-S0302283818307401-main.pdf           0   \n",
       "2                   1-s2.0-S258893112300038X-main.pdf           0   \n",
       "3                      10.1007@s00345-019-03000-5.pdf           0   \n",
       "4                                       abbod2007.pdf           0   \n",
       "5                             AI and reproductive.pdf           0   \n",
       "6   Cancer Science - 2022 - Iwamura - Machine lear...           0   \n",
       "7                                    cci.18.00080.pdf           0   \n",
       "8                                       doyle2021.pdf           0   \n",
       "9                                             HBP.pdf           0   \n",
       "10                                           HBP².pdf           0   \n",
       "11                                     icu-61-239.pdf           0   \n",
       "12                                reproductive AI.pdf           0   \n",
       "13                             s12911-021-01585-9.pdf           0   \n",
       "14                              s41585-020-0312-1.pdf           0   \n",
       "15                                  teichmann2007.pdf           0   \n",
       "16                         tju-46-supplement1-s27.pdf           0   \n",
       "17                            urolithiasis and AI.pdf           0   \n",
       "18                                 wollin1998 (1).pdf           0   \n",
       "19                                     wollin1998.pdf           0   \n",
       "\n",
       "                                               Author  \\\n",
       "0                                  Armando J. Lorenzo   \n",
       "1                               Gregory B. Auffenberg   \n",
       "2                                         Ali Sabbagh   \n",
       "3                            Rodrigo Suarez-Ibarrola    \n",
       "4                                             Unknown   \n",
       "5   Kevin Y. Chu; Daniel E. Nassau; Himanshu Arora...   \n",
       "6                                              Unkown   \n",
       "7   Sami-Ramzi Leyh-Bannurah, Zhe Tian, Pierre I. ...   \n",
       "8                                   Patrick W. Doyle    \n",
       "9                       Abolfazl Doostparast Torshizi   \n",
       "10                                             Unkown   \n",
       "11                                             Unkown   \n",
       "12            Yunfeng Guan, Aijun Zhang and Bufang Xu   \n",
       "13                                      Hesham Salem    \n",
       "14                                     Richard Naspro   \n",
       "15                                             Unkown   \n",
       "16                                             Unkown   \n",
       "17                                             Unkown   \n",
       "18                 TIM A. WOLLIN and JOHN D. DENSTEDT   \n",
       "19                 TIM A. WOLLIN and JOHN D. DENSTEDT   \n",
       "\n",
       "                                              subject  Id  \\\n",
       "0   Urology, 123 (2018) 204-209. doi:10.1016/j.uro...   0   \n",
       "1   European Urology, 75 (2019) 901-907. doi:10.10...   1   \n",
       "2   European Urology Oncology, Corrected proof. do...   2   \n",
       "3   World Journal of Urology, https://doi.org/10.1...   3   \n",
       "4                                             Unknown   4   \n",
       "5        Curr Urol Rep, doi:10.1007/s11934-019-0914-4   5   \n",
       "6                   Cancer Science 2022.113:2434-2445   6   \n",
       "7           JCO Clinical Cancer Informatics 2018.:1-9   7   \n",
       "8   World Journal of Urology, https://doi.org/10.1...   8   \n",
       "9   Computer Methods and Programs in Biomedicine, ...   9   \n",
       "10                                             Unkown  10   \n",
       "11                                             Unkown  11   \n",
       "12  Introduction: Semen quality has decreased grad...  12   \n",
       "13  BMC Medical Informatics and Decision Making, h...  13   \n",
       "14  Nature Reviews Urology, doi:10.1038/s41585-020...  14   \n",
       "15                                             Unkown  15   \n",
       "16                                             Unkown  16   \n",
       "17                                             Unkown  17   \n",
       "18  Journal of Clinical Laser Medicine & Surgery 1...  18   \n",
       "19  Journal of Clinical Laser Medicine & Surgery 1...  19   \n",
       "\n",
       "                                              Creator  \\\n",
       "0                                            Elsevier   \n",
       "1                                            Elsevier   \n",
       "2                                            Elsevier   \n",
       "3                                            Springer   \n",
       "4                                            Elsevier   \n",
       "5   Arbortext Advanced Print Publisher 9.1.440/W U...   \n",
       "6                       Adobe InDesign 15.0 (Windows)   \n",
       "7                    Adobe InDesign CC 13.0 (Windows)   \n",
       "8                                            Springer   \n",
       "9                                            Elsevier   \n",
       "10                                           Elsevier   \n",
       "11                      Adobe InDesign 14.0 (Windows)   \n",
       "12             LaTeX with hyperref package + hypdvips   \n",
       "13                      Adobe InDesign 15.0 (Windows)   \n",
       "14                                           Springer   \n",
       "15                                     FrameMaker 7.1   \n",
       "16                    Adobe InDesign 15.1 (Macintosh)   \n",
       "17                3B2 Total Publishing System 8.07f/W   \n",
       "18                                Apex CoVantage LLC.   \n",
       "19                                Apex CoVantage LLC.   \n",
       "\n",
       "                                                title  \n",
       "0   Predictive Analytics and Modeling Employing Ma...  \n",
       "1   askMUSIC: Leveraging a Clinical Registry to De...  \n",
       "2   Development and External Validation of a Machi...  \n",
       "3   Current and future applications of machine and...  \n",
       "4                      doi:10.1016/j.juro.2007.05.122  \n",
       "5     Artificial Intelligence in Reproductive Urology  \n",
       "6   Machine learning diagnosis by immunoglobulin N...  \n",
       "7   Deep Learning for Natural Language Processing ...  \n",
       "8   Machine learning applications to enhance patie...  \n",
       "9   A hybrid fuzzy-ontology based intelligent syst...  \n",
       "10                         PII: S0022-5347(05)68042-1  \n",
       "11                                             Unkown  \n",
       "12  Preliminary prediction of semen quality based ...  \n",
       "13  A systematic review of the applications of Exp...  \n",
       "14                      Urology in the time of corona  \n",
       "15                                             Unkown  \n",
       "16                                             Unkown  \n",
       "17                                             Unkown  \n",
       "18                       The Holmium Laser in Urology  \n",
       "19                       The Holmium Laser in Urology  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.fillna(\"Unkown\").replace(\"\",\"Unknown\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Cluster'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\ASUS\\anaconda3\\envs\\projet_s8\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3802\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3801\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 3802\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n\u001b[0;32m   3803\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\anaconda3\\envs\\projet_s8\\lib\\site-packages\\pandas\\_libs\\index.pyx:138\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\anaconda3\\envs\\projet_s8\\lib\\site-packages\\pandas\\_libs\\index.pyx:165\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5745\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5753\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Cluster'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m x\u001b[39m=\u001b[39m data[\u001b[39m\"\u001b[39;49m\u001b[39mCluster\u001b[39;49m\u001b[39m\"\u001b[39;49m]\n\u001b[0;32m      2\u001b[0m x\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\anaconda3\\envs\\projet_s8\\lib\\site-packages\\pandas\\core\\frame.py:3807\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3805\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mnlevels \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m   3806\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3807\u001b[0m indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns\u001b[39m.\u001b[39;49mget_loc(key)\n\u001b[0;32m   3808\u001b[0m \u001b[39mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   3809\u001b[0m     indexer \u001b[39m=\u001b[39m [indexer]\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\anaconda3\\envs\\projet_s8\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3804\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3802\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine\u001b[39m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3803\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m-> 3804\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[0;32m   3805\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[0;32m   3806\u001b[0m     \u001b[39m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3807\u001b[0m     \u001b[39m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3808\u001b[0m     \u001b[39m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3809\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Cluster'"
     ]
    }
   ],
   "source": [
    "x= data[\"Cluster\"]\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ASUS\\anaconda3\\envs\\projet_s8\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "c:\\Users\\ASUS\\anaconda3\\envs\\projet_s8\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['could', 'might', 'must', 'need', 'sha', 'wo', 'would'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#define vectorizer parameters\n",
    "data = data.reset_index(drop=True)\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=200,\n",
    "                                 stop_words= stopwords,\n",
    "                                 use_idf=True, tokenizer=tokenize_and_stem)\n",
    "\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(data[\"text\"]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 55)\t0.09501025342187114\n",
      "  (0, 111)\t0.042072211329677485\n",
      "  (0, 98)\t0.11569858115661309\n",
      "  (0, 57)\t0.024645665575136762\n",
      "  (0, 34)\t0.016858959916441155\n",
      "  (0, 13)\t0.010001732175881075\n",
      "  (0, 80)\t0.010518052832419371\n",
      "  (0, 169)\t0.009518746457391829\n",
      "  (0, 164)\t0.019037492914783658\n",
      "  (0, 53)\t0.008637295765624648\n",
      "  (0, 40)\t0.02855623937217549\n",
      "  (0, 126)\t0.016858959916441155\n",
      "  (0, 28)\t0.017274591531249297\n",
      "  (0, 191)\t0.07252040890993754\n",
      "  (0, 47)\t0.021036105664838742\n",
      "  (0, 100)\t0.0400069287035243\n",
      "  (0, 102)\t0.038074985829567316\n",
      "  (0, 76)\t0.008637295765624648\n",
      "  (0, 176)\t0.009518746457391829\n",
      "  (0, 182)\t0.027195153341226578\n",
      "  (0, 51)\t0.023343334759638422\n",
      "  (0, 38)\t0.009518746457391829\n",
      "  (0, 35)\t0.010518052832419371\n",
      "  (0, 42)\t0.013036103790075637\n",
      "  (0, 187)\t0.011671667379819211\n",
      "  :\t:\n",
      "  (18, 90)\t0.18331109204505408\n",
      "  (18, 148)\t0.07082045188832757\n",
      "  (18, 84)\t0.06504992720216958\n",
      "  (18, 112)\t0.11568636860169075\n",
      "  (18, 91)\t0.16792313928274483\n",
      "  (18, 49)\t0.20671250805080565\n",
      "  (18, 4)\t0.003077635957386652\n",
      "  (18, 106)\t0.04993015370764507\n",
      "  (18, 189)\t0.012310543829546609\n",
      "  (18, 193)\t0.01173785209865664\n",
      "  (19, 63)\t0.25933993219157103\n",
      "  (19, 85)\t0.17289328812771404\n",
      "  (19, 96)\t0.15992629151813548\n",
      "  (19, 37)\t0.7996314575906774\n",
      "  (19, 62)\t0.3284972474426566\n",
      "  (19, 90)\t0.18331109204505408\n",
      "  (19, 148)\t0.07082045188832757\n",
      "  (19, 84)\t0.06504992720216958\n",
      "  (19, 112)\t0.11568636860169075\n",
      "  (19, 91)\t0.16792313928274483\n",
      "  (19, 49)\t0.20671250805080565\n",
      "  (19, 4)\t0.003077635957386652\n",
      "  (19, 106)\t0.04993015370764507\n",
      "  (19, 189)\t0.012310543829546609\n",
      "  (19, 193)\t0.01173785209865664\n"
     ]
    }
   ],
   "source": [
    "print(tfidf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms=tfidf_vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Sum_of_squared_distances = []\n",
    "K = range(2,10)\n",
    "for k in K:\n",
    "    km = KMeans(n_clusters=k, max_iter=1000, n_init=1,random_state=42,verbose=0)\n",
    "    km = km.fit(tfidf_matrix)\n",
    "    Sum_of_squared_distances.append(km.inertia_)\n",
    "plt.plot(K, Sum_of_squared_distances, 'bx-')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Sum_of_squared_distances')\n",
    "plt.title('Elbow Method For Optimal k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_clusters(data, max_clusters=len(file)):\n",
    "    best_score = -1\n",
    "    best_k = 0\n",
    "    for k in range(2, max_clusters+1):\n",
    "        kmeans = KMeans(n_clusters=k, max_iter=1000, n_init=1, random_state=42,verbose=0)\n",
    "        kmeans.fit(data)\n",
    "        score = silhouette_score(data, kmeans.labels_)\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_k = k\n",
    "    return best_k\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_116292\\1789472525.py:6: ConvergenceWarning: Number of distinct clusters (19) found smaller than n_clusters (20). Possibly due to duplicate points in X.\n",
      "  kmeans.fit(data)\n"
     ]
    }
   ],
   "source": [
    "nbr_cluster=find_best_clusters(tfidf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>KMeans(max_iter=100, n_init=1, random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KMeans</label><div class=\"sk-toggleable__content\"><pre>KMeans(max_iter=100, n_init=1, random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "KMeans(max_iter=100, n_init=1, random_state=42)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model = KMeans(n_clusters=nbr_cluster, init='k-means++', \n",
    "            max_iter=100, n_init=1, verbose=0, random_state=42)\n",
    "\n",
    "model.fit_transform(tfidf_matrix)\n",
    "model.fit(tfidf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Cluster'] = model.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = KMeans(n_clusters=nbr_cluster, max_iter=200, n_init=1,random_state=42,verbose=0).fit_predict(tfidf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "totalvocab_stemmed = []\n",
    "totalvocab_tokenized = []\n",
    "for i in data[\"text\"]:\n",
    "    allwords_stemmed = tokenize_and_stem(i)\n",
    "    totalvocab_stemmed.extend(allwords_stemmed) #extend the 'totalvocab_stemmed' list\n",
    "    \n",
    "    allwords_tokenized = tokenize_only(i)\n",
    "    totalvocab_tokenized.extend(allwords_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_frame = pd.DataFrame({'words': totalvocab_tokenized}, index = totalvocab_stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top terms per cluster:\n",
      "\n",
      "Cluster 0 words: glycan, specifically, scored, detected, bca, disease, cancer, urology, auc, using, respectively, bph, et, et, sensitivity, cell, prostate, datasets, al, https, diagnostic, tract, prostate, urinary, value, lower, bladder, analysis, ml, carcinoma, renal, patient, medicine, article, data, shows, bladder, could, also, benign, roc, tumors, system, university, psa, predictive, learning, diagnosis, study, total, cases, results, machine, validating, applications, modeling, several, machine, evaluated, accuracy, pathologic, combining, important, therapy, difference, classify, status, clinical, research, information, need, approaches, identify, selected, staging, represent, patterns, authors, time, high, based, require, groups, treatments, table, reporting, med, supporting, training, improve, number, tests, biopsy, included, outcome, medical, available, two, obtained, risk,\n",
      "Cluster 1 words: predictive, et, using, et, patient, ml, al, data, ann, modeling, stone, imaging, cancer, learning, urology, renal, machine, network, algorithms, ai, accuracy, neural, clinical, prostate, svm, system, study, surgical, artificial, neural, training, outcome, machine, analysis, development, bladder, lr, based, applications, datasets, cell, grade, cap, methods, treatments, settings, comput, performed, auc, results, urol, intelligence, improve, features, ct, information, prostate, tumors, recurrence, sp, diagnosis, variables, validating, surgery, input, tests, artificial, groups, supporting, experts, disease, staging, bladder, applying, research, accurate, one, https, se, decision, identify, care, medical, scored, evaluated, techniques, artificial, diagnostic, carcinoma, statistical, sensitivity, artificial, psa, pathologic, need, obtained, compared, med, specifically, medicine,\n",
      "Cluster 2 words: data, patient, modeling, cancer, prostate, using, prostate, gleason, validating, clinical, et, https, et, treatments, urology, lymph, lymph, al, node, xgboost, radical, tumors, predictive, patterns, training, information, ml, pathologic, development, learning, biopsy, settings, table, staging, health, volume, machine, urol, performed, standard, variables, lr, study, represent, quality, men, medical, may, reporting, approaches, tool, machine, fig, providing, university, auc, based, therapy, process, improve, risk, generating, surgical, specifically, physicians, available, difference, value, important, supporting, statistical, analysis, outcome, tests, results, authors, tion, number, article, following, care, decision, research, included, surgery, compared, scored, positive, psa, ann, methods, total, imaging, could, neural, potential, accuracy, respectively, system, applying,\n",
      "Cluster 3 words: lasers, tissue, surgical, may, generating, using, prostate, urol, surgery, require, providing, process, high, results, level, available, fig, need, applications, stone, development, nm, bph, tion, cases, time, system, currently, lower, medical, ing, following, urology, important, treatments, represent, mm, bladder, output, standard, risk, features, could, applying, settings, rm, volume, benign, pathologic, urinary, human, factors, https, value, compared, age, combining, reviewed, techniques, two, respectively, improve, also, tool, large, number, modeling, included, identify, analysis, selected, several, patient, clinical, one, performed, outcome, fucnlrn, artificial, fuzzy, clm, artificial, fucnlrn, artificial, artificial, auc, article, gleason, approaches, ann, experts, glycan, grade, groups, health, ann, algorithms, al, ai, accuracy,\n",
      "Cluster 4 words: clm, fucnlrn, fucnlrn, df, lm, lag, ln, lz, nm, rm, las, mm, university, using, al, factors, generating, fuzzy, lower, lr, following, fig, features, evaluated, experts, glycan, et, et, disease, difference, lymph, diagnostic, gleason, groups, grade, learning, lasers, large, level, knowledge, intelligence, input, ing, information, included, improve, important, imaging, diagnosis, human, https, high, health, identify, years, development, approaches, based, available, authors, auc, artificial, artificial, artificial, artificial, article, applications, benign, applying, ann, ann, analysis, also, algorithms, ai, age, accuracy, bca, biopsy, detected, clinical, decision, datasets, data, currently, ct, could, comput, compared, combining, classify, bladder, centre, cell, cases, care, carcinoma, cap, cancer, bph,\n",
      "Cluster 5 words: patient, network, predictive, neural, using, neural, tests, prostate, ann, settings, value, outcome, regression, results, diagnostic, modeling, number, node, artificial, study, bladder, training, volume, men, urinary, artificial, urol, tract, sensitivity, artificial, specifically, auc, treatments, variables, research, benign, information, shows, scored, standard, lower, may, quality, performed, age, compared, diagnosis, evaluated, input, one, analysis, urology, medical, clinical, available, high, difference, combining, represent, improve, methods, data, fig, several, cancer, approaches, identify, post, table, total, university, output, psa, potential, las, based, large, validating, also, providing, artificial, roc, important, development, selected, disease, et, et, included, al, gleason, pathologic, positive, groups, classify, ml, intelligence, accurate, process, staging,\n",
      "Cluster 6 words: sperm, semen, ai, xgboost, predictive, smoking, artificial, factors, quality, artificial, modeling, shows, medicine, intelligence, et, et, auc, al, using, regression, learning, urology, data, important, study, applications, men, algorithms, analysis, machine, machine, variables, potential, patient, status, volume, age, parameters, human, research, ann, network, neural, progress, comput, total, identify, urol, performed, neural, process, based, results, methods, may, cancer, risk, currently, artificial, artificial, medical, lower, selected, roc, years, evaluated, datasets, diagnostic, clinical, tests, improve, features, respectively, reporting, imaging, reviewed, included, accuracy, med, prostate, training, system, input, need, level, university, information, validating, following, development, health, could, available, https, article, grade, providing, accurate, authors, experts,\n",
      "Cluster 7 words: fuzzy, system, bph, knowledge, fig, prostate, patient, experts, represent, physicians, time, therapy, using, development, based, treatments, results, high, medical, variables, output, two, value, level, modeling, benign, intelligence, several, prostate, cancer, methods, comput, process, approaches, study, obtained, scored, settings, applications, evaluated, data, table, features, urinary, surgical, difference, staging, imaging, men, statistical, research, cases, input, number, ing, quality, following, generating, techniques, information, network, may, included, urology, groups, age, validating, diagnosis, analysis, one, respectively, large, neural, neural, factors, disease, performed, decision, also, accuracy, years, parameters, tract, lower, detected, university, medicine, risk, authors, tion, tests, applying, available, health, compared, combining, algorithms, need, providing, progress,\n"
     ]
    }
   ],
   "source": [
    "print(\"Top terms per cluster:\")\n",
    "print()\n",
    "#sort cluster centers by proximity to centroid\n",
    "order_centroids = km.cluster_centers_.argsort()[:, ::-1] \n",
    "\n",
    "for i in range(nbr_cluster):\n",
    "    print(\"Cluster %d words:\" % i, end='')\n",
    "    \n",
    "    for ind in order_centroids[i, :100]:\n",
    "        print(' %s' % vocab_frame.loc[terms[ind].split(' ')].values.tolist()[0][0], end=',')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0:\n",
      " glycan\n",
      " specif\n",
      " score\n",
      " detect\n",
      " bca\n",
      " diseas\n",
      " cancer\n",
      " urolog\n",
      " auc\n",
      " use\n",
      " respect\n",
      " bph\n",
      " et\n",
      " et al\n",
      " sensit\n",
      " cell\n",
      " prostat\n",
      " dataset\n",
      " al\n",
      " http\n",
      " diagnost\n",
      " tract\n",
      " prostat cancer\n",
      " urinari\n",
      " valu\n",
      " lower\n",
      " bladder cancer\n",
      " analysi\n",
      " ml\n",
      " carcinoma\n",
      " renal\n",
      " patient\n",
      " medicin\n",
      " articl\n",
      " data\n",
      " show\n",
      " bladder\n",
      " could\n",
      " also\n",
      " benign\n",
      " roc\n",
      " tumor\n",
      " system\n",
      " univers\n",
      " psa\n",
      " predict\n",
      " learn\n",
      " diagnosi\n",
      " studi\n",
      " total\n",
      " case\n",
      " result\n",
      " machin\n",
      " valid\n",
      " applic\n",
      " model\n",
      " sever\n",
      " machin learn\n",
      " evalu\n",
      " accuraci\n",
      " patholog\n",
      " combin\n",
      " import\n",
      " therapi\n",
      " differ\n",
      " classifi\n",
      " statu\n",
      " clinic\n",
      " research\n",
      " inform\n",
      " need\n",
      " approach\n",
      " identifi\n",
      " select\n",
      " stage\n",
      " repres\n",
      " pattern\n",
      " author\n",
      " time\n",
      " high\n",
      " base\n",
      " requir\n",
      " group\n",
      " treatment\n",
      " tabl\n",
      " report\n",
      " med\n",
      " support\n",
      " train\n",
      " improv\n",
      " number\n",
      " test\n",
      " biopsi\n",
      " includ\n",
      " outcom\n",
      " medic\n",
      " avail\n",
      " two\n",
      " obtain\n",
      " risk\n",
      "Cluster 1:\n",
      " predict\n",
      " et\n",
      " use\n",
      " et al\n",
      " patient\n",
      " ml\n",
      " al\n",
      " data\n",
      " ann\n",
      " model\n",
      " stone\n",
      " imag\n",
      " cancer\n",
      " learn\n",
      " urolog\n",
      " renal\n",
      " machin\n",
      " network\n",
      " algorithm\n",
      " ai\n",
      " accuraci\n",
      " neural\n",
      " clinic\n",
      " prostat\n",
      " svm\n",
      " system\n",
      " studi\n",
      " surgic\n",
      " artifici\n",
      " neural network\n",
      " train\n",
      " outcom\n",
      " machin learn\n",
      " analysi\n",
      " develop\n",
      " bladder\n",
      " lr\n",
      " base\n",
      " applic\n",
      " dataset\n",
      " cell\n",
      " grade\n",
      " cap\n",
      " method\n",
      " treatment\n",
      " set\n",
      " comput\n",
      " perform\n",
      " auc\n",
      " result\n",
      " urol\n",
      " intellig\n",
      " improv\n",
      " featur\n",
      " ct\n",
      " inform\n",
      " prostat cancer\n",
      " tumor\n",
      " recurr\n",
      " sp\n",
      " diagnosi\n",
      " variabl\n",
      " valid\n",
      " surgeri\n",
      " input\n",
      " test\n",
      " artifici intellig\n",
      " group\n",
      " support\n",
      " expert\n",
      " diseas\n",
      " stage\n",
      " bladder cancer\n",
      " appli\n",
      " research\n",
      " accur\n",
      " one\n",
      " http\n",
      " se\n",
      " decis\n",
      " identifi\n",
      " care\n",
      " medic\n",
      " score\n",
      " evalu\n",
      " techniqu\n",
      " artifici neural\n",
      " diagnost\n",
      " carcinoma\n",
      " statist\n",
      " sensit\n",
      " artifici neural network\n",
      " psa\n",
      " patholog\n",
      " need\n",
      " obtain\n",
      " compar\n",
      " med\n",
      " specif\n",
      " medicin\n",
      "Cluster 2:\n",
      " data\n",
      " patient\n",
      " model\n",
      " cancer\n",
      " prostat\n",
      " use\n",
      " prostat cancer\n",
      " gleason\n",
      " valid\n",
      " clinic\n",
      " et\n",
      " http\n",
      " et al\n",
      " treatment\n",
      " urolog\n",
      " lymph node\n",
      " lymph\n",
      " al\n",
      " node\n",
      " xgboost\n",
      " radic\n",
      " tumor\n",
      " predict\n",
      " pattern\n",
      " train\n",
      " inform\n",
      " ml\n",
      " patholog\n",
      " develop\n",
      " learn\n",
      " biopsi\n",
      " set\n",
      " tabl\n",
      " stage\n",
      " health\n",
      " volum\n",
      " machin\n",
      " urol\n",
      " perform\n",
      " standard\n",
      " variabl\n",
      " lr\n",
      " studi\n",
      " repres\n",
      " qualiti\n",
      " men\n",
      " medic\n",
      " may\n",
      " report\n",
      " approach\n",
      " tool\n",
      " machin learn\n",
      " fig\n",
      " provid\n",
      " univers\n",
      " auc\n",
      " base\n",
      " therapi\n",
      " process\n",
      " improv\n",
      " risk\n",
      " gener\n",
      " surgic\n",
      " specif\n",
      " physician\n",
      " avail\n",
      " differ\n",
      " valu\n",
      " import\n",
      " support\n",
      " statist\n",
      " analysi\n",
      " outcom\n",
      " test\n",
      " result\n",
      " author\n",
      " tion\n",
      " number\n",
      " articl\n",
      " follow\n",
      " care\n",
      " decis\n",
      " research\n",
      " includ\n",
      " surgeri\n",
      " compar\n",
      " score\n",
      " posit\n",
      " psa\n",
      " ann\n",
      " method\n",
      " total\n",
      " imag\n",
      " could\n",
      " neural\n",
      " potenti\n",
      " accuraci\n",
      " respect\n",
      " system\n",
      " appli\n",
      "Cluster 3:\n",
      " laser\n",
      " tissu\n",
      " surgic\n",
      " may\n",
      " gener\n",
      " use\n",
      " prostat\n",
      " urol\n",
      " surgeri\n",
      " requir\n",
      " provid\n",
      " process\n",
      " high\n",
      " result\n",
      " level\n",
      " avail\n",
      " fig\n",
      " need\n",
      " applic\n",
      " stone\n",
      " develop\n",
      " nm\n",
      " bph\n",
      " tion\n",
      " case\n",
      " time\n",
      " system\n",
      " current\n",
      " lower\n",
      " medic\n",
      " ing\n",
      " follow\n",
      " urolog\n",
      " import\n",
      " treatment\n",
      " repres\n",
      " mm\n",
      " bladder\n",
      " output\n",
      " standard\n",
      " risk\n",
      " featur\n",
      " could\n",
      " appli\n",
      " set\n",
      " rm\n",
      " volum\n",
      " benign\n",
      " patholog\n",
      " urinari\n",
      " human\n",
      " factor\n",
      " http\n",
      " valu\n",
      " compar\n",
      " age\n",
      " combin\n",
      " review\n",
      " techniqu\n",
      " two\n",
      " respect\n",
      " improv\n",
      " also\n",
      " tool\n",
      " larg\n",
      " number\n",
      " model\n",
      " includ\n",
      " identifi\n",
      " analysi\n",
      " select\n",
      " sever\n",
      " patient\n",
      " clinic\n",
      " one\n",
      " perform\n",
      " outcom\n",
      " fucnlrn clm\n",
      " artifici\n",
      " fuzzi\n",
      " clm\n",
      " artifici intellig\n",
      " fucnlrn\n",
      " artifici neural\n",
      " artifici neural network\n",
      " auc\n",
      " articl\n",
      " gleason\n",
      " approach\n",
      " ann cap\n",
      " expert\n",
      " glycan\n",
      " grade\n",
      " group\n",
      " health\n",
      " ann\n",
      " algorithm\n",
      " al\n",
      " ai\n",
      " accuraci\n",
      "Cluster 4:\n",
      " clm\n",
      " fucnlrn\n",
      " fucnlrn clm\n",
      " df\n",
      " lm\n",
      " lag\n",
      " ln\n",
      " lz\n",
      " nm\n",
      " rm\n",
      " la\n",
      " mm\n",
      " univers\n",
      " use\n",
      " al\n",
      " factor\n",
      " gener\n",
      " fuzzi\n",
      " lower\n",
      " lr\n",
      " follow\n",
      " fig\n",
      " featur\n",
      " evalu\n",
      " expert\n",
      " glycan\n",
      " et al\n",
      " et\n",
      " diseas\n",
      " differ\n",
      " lymph\n",
      " diagnost\n",
      " gleason\n",
      " group\n",
      " grade\n",
      " learn\n",
      " laser\n",
      " larg\n",
      " level\n",
      " knowledg\n",
      " intellig\n",
      " input\n",
      " ing\n",
      " inform\n",
      " includ\n",
      " improv\n",
      " import\n",
      " imag\n",
      " diagnosi\n",
      " human\n",
      " http\n",
      " high\n",
      " health\n",
      " identifi\n",
      " year\n",
      " develop\n",
      " approach\n",
      " base\n",
      " avail\n",
      " author\n",
      " auc\n",
      " artifici neural network\n",
      " artifici neural\n",
      " artifici intellig\n",
      " artifici\n",
      " articl\n",
      " applic\n",
      " benign\n",
      " appli\n",
      " ann cap\n",
      " ann\n",
      " analysi\n",
      " also\n",
      " algorithm\n",
      " ai\n",
      " age\n",
      " accuraci\n",
      " bca\n",
      " biopsi\n",
      " detect\n",
      " clinic\n",
      " decis\n",
      " dataset\n",
      " data\n",
      " current\n",
      " ct\n",
      " could\n",
      " comput\n",
      " compar\n",
      " combin\n",
      " classifi\n",
      " bladder\n",
      " centr\n",
      " cell\n",
      " case\n",
      " care\n",
      " carcinoma\n",
      " cap\n",
      " cancer\n",
      " bph\n",
      "Cluster 5:\n",
      " patient\n",
      " network\n",
      " predict\n",
      " neural\n",
      " use\n",
      " neural network\n",
      " test\n",
      " prostat\n",
      " ann\n",
      " set\n",
      " valu\n",
      " outcom\n",
      " regress\n",
      " result\n",
      " diagnost\n",
      " model\n",
      " number\n",
      " node\n",
      " artifici\n",
      " studi\n",
      " bladder\n",
      " train\n",
      " volum\n",
      " men\n",
      " urinari\n",
      " artifici neural\n",
      " urol\n",
      " tract\n",
      " sensit\n",
      " artifici neural network\n",
      " specif\n",
      " auc\n",
      " treatment\n",
      " variabl\n",
      " research\n",
      " benign\n",
      " inform\n",
      " show\n",
      " score\n",
      " standard\n",
      " lower\n",
      " may\n",
      " qualiti\n",
      " perform\n",
      " age\n",
      " compar\n",
      " diagnosi\n",
      " evalu\n",
      " input\n",
      " one\n",
      " analysi\n",
      " urolog\n",
      " medic\n",
      " clinic\n",
      " avail\n",
      " high\n",
      " differ\n",
      " combin\n",
      " repres\n",
      " improv\n",
      " method\n",
      " data\n",
      " fig\n",
      " sever\n",
      " cancer\n",
      " approach\n",
      " identifi\n",
      " post\n",
      " tabl\n",
      " total\n",
      " univers\n",
      " output\n",
      " psa\n",
      " potenti\n",
      " la\n",
      " base\n",
      " larg\n",
      " valid\n",
      " also\n",
      " provid\n",
      " artifici intellig\n",
      " roc\n",
      " import\n",
      " develop\n",
      " select\n",
      " diseas\n",
      " et\n",
      " et al\n",
      " includ\n",
      " al\n",
      " gleason\n",
      " patholog\n",
      " posit\n",
      " group\n",
      " classifi\n",
      " ml\n",
      " intellig\n",
      " accur\n",
      " process\n",
      " stage\n",
      "Cluster 6:\n",
      " sperm\n",
      " semen\n",
      " ai\n",
      " xgboost\n",
      " predict\n",
      " smoke\n",
      " artifici\n",
      " factor\n",
      " qualiti\n",
      " artifici intellig\n",
      " model\n",
      " show\n",
      " medicin\n",
      " intellig\n",
      " et\n",
      " et al\n",
      " auc\n",
      " al\n",
      " use\n",
      " regress\n",
      " learn\n",
      " urolog\n",
      " data\n",
      " import\n",
      " studi\n",
      " applic\n",
      " men\n",
      " algorithm\n",
      " analysi\n",
      " machin\n",
      " machin learn\n",
      " variabl\n",
      " potenti\n",
      " patient\n",
      " statu\n",
      " volum\n",
      " age\n",
      " paramet\n",
      " human\n",
      " research\n",
      " ann\n",
      " network\n",
      " neural\n",
      " progress\n",
      " comput\n",
      " total\n",
      " identifi\n",
      " urol\n",
      " perform\n",
      " neural network\n",
      " process\n",
      " base\n",
      " result\n",
      " method\n",
      " may\n",
      " cancer\n",
      " risk\n",
      " current\n",
      " artifici neural network\n",
      " artifici neural\n",
      " medic\n",
      " lower\n",
      " select\n",
      " roc\n",
      " year\n",
      " evalu\n",
      " dataset\n",
      " diagnost\n",
      " clinic\n",
      " test\n",
      " improv\n",
      " featur\n",
      " respect\n",
      " report\n",
      " imag\n",
      " review\n",
      " includ\n",
      " accuraci\n",
      " med\n",
      " prostat\n",
      " train\n",
      " system\n",
      " input\n",
      " need\n",
      " level\n",
      " univers\n",
      " inform\n",
      " valid\n",
      " follow\n",
      " develop\n",
      " health\n",
      " could\n",
      " avail\n",
      " http\n",
      " articl\n",
      " grade\n",
      " provid\n",
      " accur\n",
      " author\n",
      " expert\n",
      "Cluster 7:\n",
      " fuzzi\n",
      " system\n",
      " bph\n",
      " knowledg\n",
      " fig\n",
      " prostat\n",
      " patient\n",
      " expert\n",
      " repres\n",
      " physician\n",
      " time\n",
      " therapi\n",
      " use\n",
      " develop\n",
      " base\n",
      " treatment\n",
      " result\n",
      " high\n",
      " medic\n",
      " variabl\n",
      " output\n",
      " two\n",
      " valu\n",
      " level\n",
      " model\n",
      " benign\n",
      " intellig\n",
      " sever\n",
      " prostat cancer\n",
      " cancer\n",
      " method\n",
      " comput\n",
      " process\n",
      " approach\n",
      " studi\n",
      " obtain\n",
      " score\n",
      " set\n",
      " applic\n",
      " evalu\n",
      " data\n",
      " tabl\n",
      " featur\n",
      " urinari\n",
      " surgic\n",
      " differ\n",
      " stage\n",
      " imag\n",
      " men\n",
      " statist\n",
      " research\n",
      " case\n",
      " input\n",
      " number\n",
      " ing\n",
      " qualiti\n",
      " follow\n",
      " gener\n",
      " techniqu\n",
      " inform\n",
      " network\n",
      " may\n",
      " includ\n",
      " urolog\n",
      " group\n",
      " age\n",
      " valid\n",
      " diagnosi\n",
      " analysi\n",
      " one\n",
      " respect\n",
      " larg\n",
      " neural\n",
      " neural network\n",
      " factor\n",
      " diseas\n",
      " perform\n",
      " decis\n",
      " also\n",
      " accuraci\n",
      " year\n",
      " paramet\n",
      " tract\n",
      " lower\n",
      " detect\n",
      " univers\n",
      " medicin\n",
      " risk\n",
      " author\n",
      " tion\n",
      " test\n",
      " appli\n",
      " avail\n",
      " health\n",
      " compar\n",
      " combin\n",
      " algorithm\n",
      " need\n",
      " provid\n",
      " progress\n"
     ]
    }
   ],
   "source": [
    "cluster = {}\n",
    "for i in range(nbr_cluster):\n",
    "    print(\"Cluster %d:\" % i)\n",
    "    cluster[i] = []  # create a new list for each cluster\n",
    "    for ind in order_centroids[i, :100]:\n",
    "        cluster[i].append(terms[ind])\n",
    "        print(' %s' % terms[ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'in <string>' requires string as left operand, not list",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[55], line 36\u001b[0m\n\u001b[0;32m     34\u001b[0m or_matches \u001b[39m=\u001b[39m []\n\u001b[0;32m     35\u001b[0m \u001b[39mfor\u001b[39;00m w \u001b[39min\u001b[39;00m k\u001b[39m.\u001b[39msplit(\u001b[39m\"\u001b[39m\u001b[39mor\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m---> 36\u001b[0m     found \u001b[39m=\u001b[39m \u001b[39many\u001b[39m([t \u001b[39min\u001b[39;00m \u001b[39mstr\u001b[39m(PorterStemmer()\u001b[39m.\u001b[39mstem(data[\u001b[39m\"\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m\"\u001b[39m][i]\u001b[39m.\u001b[39mlower())) \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m and_keywords[\u001b[39m0\u001b[39m]])\n\u001b[0;32m     37\u001b[0m     or_matches\u001b[39m.\u001b[39mappend(found)\n\u001b[0;32m     38\u001b[0m matches\u001b[39m.\u001b[39mappend(\u001b[39many\u001b[39m(or_matches))\n",
      "Cell \u001b[1;32mIn[55], line 36\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     34\u001b[0m or_matches \u001b[39m=\u001b[39m []\n\u001b[0;32m     35\u001b[0m \u001b[39mfor\u001b[39;00m w \u001b[39min\u001b[39;00m k\u001b[39m.\u001b[39msplit(\u001b[39m\"\u001b[39m\u001b[39mor\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m---> 36\u001b[0m     found \u001b[39m=\u001b[39m \u001b[39many\u001b[39m([t \u001b[39min\u001b[39;49;00m \u001b[39mstr\u001b[39;49m(PorterStemmer()\u001b[39m.\u001b[39;49mstem(data[\u001b[39m\"\u001b[39;49m\u001b[39mtext\u001b[39;49m\u001b[39m\"\u001b[39;49m][i]\u001b[39m.\u001b[39;49mlower())) \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m and_keywords[\u001b[39m0\u001b[39m]])\n\u001b[0;32m     37\u001b[0m     or_matches\u001b[39m.\u001b[39mappend(found)\n\u001b[0;32m     38\u001b[0m matches\u001b[39m.\u001b[39mappend(\u001b[39many\u001b[39m(or_matches))\n",
      "\u001b[1;31mTypeError\u001b[0m: 'in <string>' requires string as left operand, not list"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "\n",
    "query = \"(xgboost or fragmentation) \"\n",
    "\n",
    "# define a dictionary of synonyms\n",
    "synonyms = {}\n",
    "for term in re.findall(r'\\b\\w+\\b', query):\n",
    "    synonyms[term] = set()\n",
    "    for syn in wordnet.synsets(term):\n",
    "        for lemma in syn.lemmas():\n",
    "            synonyms[term].add(lemma.name())\n",
    "\n",
    "or_keywords = [k.strip() for k in query.split(\"and\")]\n",
    "and_keywords = []\n",
    "for match in re.findall(r'\\((.*?)\\)|(\\b\\w+\\b)|(\\b(or)\\b)|(\\b(and)\\b)', query):\n",
    "    keyword = \"\".join([x for x in match if x]).strip().lower()\n",
    "    if keyword:\n",
    "        if keyword != \"or\" and \"or\" in keyword:\n",
    "            and_keywords.append([[PorterStemmer().stem(w.strip().lower())]+list(synonyms.get(w.strip().lower(), set())) for w in keyword.split(\"or\")])\n",
    "        elif keyword != \"and\" and \"and\" in keyword:\n",
    "            and_keywords.append([[PorterStemmer().stem(w.strip().lower())]+list(synonyms.get(w.strip().lower(), set())) for w in keyword.split(\"and\")])\n",
    "        else:\n",
    "            and_keywords.append([[PorterStemmer().stem(keyword)]+list(synonyms.get(keyword, set()))])\n",
    "\n",
    "and_keywords = [x for x in and_keywords if  x != [['or']] and x!=[['and']]]\n",
    "\n",
    "for i in range(len(data[\"text\"])):\n",
    "    matches = []\n",
    "    if len(or_keywords) == 1:\n",
    "        k = or_keywords[0]\n",
    "        or_matches = []\n",
    "        for w in k.split(\"or\"):\n",
    "            found = any([t in str(PorterStemmer().stem(data[\"text\"][i].lower())) for t in and_keywords[0]])\n",
    "            or_matches.append(found)\n",
    "        matches.append(any(or_matches))\n",
    "    elif len(and_keywords) == 1:\n",
    "        k = and_keywords[0]\n",
    "        and_matches = []\n",
    "        for w in k:\n",
    "            found = any([t in str(PorterStemmer().stem(data[\"text\"][i].lower())) for t in w])\n",
    "            and_matches.append(found)\n",
    "        matches.append(all(and_matches))\n",
    "    else:\n",
    "        for k in and_keywords:\n",
    "            or_matches = []\n",
    "            for w in k:\n",
    "                found = any([t in str(PorterStemmer().stem(data[\"text\"][i].lower())) for t in w])\n",
    "                or_matches.append(found)\n",
    "            matches.append(any(or_matches))\n",
    "        #matches = [m for m in matches if m]\n",
    " \n",
    "    if all(matches):\n",
    "        print(\"Match found! at pdf\",data[\"titre\"][i])\n",
    "    else:\n",
    "        print(\"No match found at pdf\",data[\"titre\"][i])\n",
    "    print(matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[['xgboost'], ['fragment', 'atomization', 'fragmentation', 'atomisation']], [['sperm', 'spermatozoan', 'spermatozoon', 'sperm', 'sperm_cell']], [['nephrolithiasi', 'nephrolithiasis', 'renal_lithiasis']]]\n",
      "['(xgboost or fragmentation)', 'sperm', 'nephrolithiasis']\n",
      "['xgboost']\n",
      "['fragment', 'atomization', 'fragmentation', 'atomisation']\n",
      "['sperm', 'spermatozoan', 'spermatozoon', 'sperm', 'sperm_cell']\n",
      "['nephrolithiasi', 'nephrolithiasis', 'renal_lithiasis']\n",
      "['xgboost']\n",
      "['fragment', 'atomization', 'fragmentation', 'atomisation']\n",
      "['sperm', 'spermatozoan', 'spermatozoon', 'sperm', 'sperm_cell']\n",
      "['nephrolithiasi', 'nephrolithiasis', 'renal_lithiasis']\n",
      "['xgboost']\n",
      "['fragment', 'atomization', 'fragmentation', 'atomisation']\n",
      "['sperm', 'spermatozoan', 'spermatozoon', 'sperm', 'sperm_cell']\n",
      "['nephrolithiasi', 'nephrolithiasis', 'renal_lithiasis']\n",
      "No match found at pdf 1-s2.0-S0090429518305971-main.pdf\n",
      "[False, False, False]\n",
      "['xgboost']\n",
      "['fragment', 'atomization', 'fragmentation', 'atomisation']\n",
      "['sperm', 'spermatozoan', 'spermatozoon', 'sperm', 'sperm_cell']\n",
      "['nephrolithiasi', 'nephrolithiasis', 'renal_lithiasis']\n",
      "['xgboost']\n",
      "['fragment', 'atomization', 'fragmentation', 'atomisation']\n",
      "['sperm', 'spermatozoan', 'spermatozoon', 'sperm', 'sperm_cell']\n",
      "['nephrolithiasi', 'nephrolithiasis', 'renal_lithiasis']\n",
      "['xgboost']\n",
      "['fragment', 'atomization', 'fragmentation', 'atomisation']\n",
      "['sperm', 'spermatozoan', 'spermatozoon', 'sperm', 'sperm_cell']\n",
      "['nephrolithiasi', 'nephrolithiasis', 'renal_lithiasis']\n",
      "No match found at pdf 1-s2.0-S0302283818307401-main.pdf\n",
      "[False, False, False]\n",
      "['xgboost']\n",
      "['fragment', 'atomization', 'fragmentation', 'atomisation']\n",
      "['sperm', 'spermatozoan', 'spermatozoon', 'sperm', 'sperm_cell']\n",
      "['nephrolithiasi', 'nephrolithiasis', 'renal_lithiasis']\n",
      "['xgboost']\n",
      "['fragment', 'atomization', 'fragmentation', 'atomisation']\n",
      "['sperm', 'spermatozoan', 'spermatozoon', 'sperm', 'sperm_cell']\n",
      "['nephrolithiasi', 'nephrolithiasis', 'renal_lithiasis']\n",
      "['xgboost']\n",
      "['fragment', 'atomization', 'fragmentation', 'atomisation']\n",
      "['sperm', 'spermatozoan', 'spermatozoon', 'sperm', 'sperm_cell']\n",
      "['nephrolithiasi', 'nephrolithiasis', 'renal_lithiasis']\n",
      "No match found at pdf 1-s2.0-S258893112300038X-main.pdf\n",
      "[False, False, False]\n",
      "['xgboost']\n",
      "['fragment', 'atomization', 'fragmentation', 'atomisation']\n",
      "['sperm', 'spermatozoan', 'spermatozoon', 'sperm', 'sperm_cell']\n",
      "['nephrolithiasi', 'nephrolithiasis', 'renal_lithiasis']\n",
      "['xgboost']\n",
      "['fragment', 'atomization', 'fragmentation', 'atomisation']\n",
      "['sperm', 'spermatozoan', 'spermatozoon', 'sperm', 'sperm_cell']\n",
      "['nephrolithiasi', 'nephrolithiasis', 'renal_lithiasis']\n",
      "['xgboost']\n",
      "['fragment', 'atomization', 'fragmentation', 'atomisation']\n",
      "['sperm', 'spermatozoan', 'spermatozoon', 'sperm', 'sperm_cell']\n",
      "['nephrolithiasi', 'nephrolithiasis', 'renal_lithiasis']\n",
      "Match found! at pdf 10.1007@s00345-019-03000-5.pdf\n",
      "[True, True, True]\n",
      "['xgboost']\n",
      "['fragment', 'atomization', 'fragmentation', 'atomisation']\n",
      "['sperm', 'spermatozoan', 'spermatozoon', 'sperm', 'sperm_cell']\n",
      "['nephrolithiasi', 'nephrolithiasis', 'renal_lithiasis']\n",
      "['xgboost']\n",
      "['fragment', 'atomization', 'fragmentation', 'atomisation']\n",
      "['sperm', 'spermatozoan', 'spermatozoon', 'sperm', 'sperm_cell']\n",
      "['nephrolithiasi', 'nephrolithiasis', 'renal_lithiasis']\n",
      "['xgboost']\n",
      "['fragment', 'atomization', 'fragmentation', 'atomisation']\n",
      "['sperm', 'spermatozoan', 'spermatozoon', 'sperm', 'sperm_cell']\n",
      "['nephrolithiasi', 'nephrolithiasis', 'renal_lithiasis']\n",
      "No match found at pdf abbod2007.pdf\n",
      "[False, False, False]\n",
      "['xgboost']\n",
      "['fragment', 'atomization', 'fragmentation', 'atomisation']\n",
      "['sperm', 'spermatozoan', 'spermatozoon', 'sperm', 'sperm_cell']\n",
      "['nephrolithiasi', 'nephrolithiasis', 'renal_lithiasis']\n",
      "['xgboost']\n",
      "['fragment', 'atomization', 'fragmentation', 'atomisation']\n",
      "['sperm', 'spermatozoan', 'spermatozoon', 'sperm', 'sperm_cell']\n",
      "['nephrolithiasi', 'nephrolithiasis', 'renal_lithiasis']\n",
      "['xgboost']\n",
      "['fragment', 'atomization', 'fragmentation', 'atomisation']\n",
      "['sperm', 'spermatozoan', 'spermatozoon', 'sperm', 'sperm_cell']\n",
      "['nephrolithiasi', 'nephrolithiasis', 'renal_lithiasis']\n",
      "Match found! at pdf AI and reproductive.pdf\n",
      "[True, True, True]\n",
      "['xgboost']\n",
      "['fragment', 'atomization', 'fragmentation', 'atomisation']\n",
      "['sperm', 'spermatozoan', 'spermatozoon', 'sperm', 'sperm_cell']\n",
      "['nephrolithiasi', 'nephrolithiasis', 'renal_lithiasis']\n",
      "['xgboost']\n",
      "['fragment', 'atomization', 'fragmentation', 'atomisation']\n",
      "['sperm', 'spermatozoan', 'spermatozoon', 'sperm', 'sperm_cell']\n",
      "['nephrolithiasi', 'nephrolithiasis', 'renal_lithiasis']\n",
      "['xgboost']\n",
      "['fragment', 'atomization', 'fragmentation', 'atomisation']\n",
      "['sperm', 'spermatozoan', 'spermatozoon', 'sperm', 'sperm_cell']\n",
      "['nephrolithiasi', 'nephrolithiasis', 'renal_lithiasis']\n",
      "No match found at pdf Cancer Science - 2022 - Iwamura - Machine learning diagnosis by immunoglobulin N‐glycan signatures for precision diagnosis.pdf\n",
      "[False, False, False]\n",
      "['xgboost']\n",
      "['fragment', 'atomization', 'fragmentation', 'atomisation']\n",
      "['sperm', 'spermatozoan', 'spermatozoon', 'sperm', 'sperm_cell']\n",
      "['nephrolithiasi', 'nephrolithiasis', 'renal_lithiasis']\n",
      "['xgboost']\n",
      "['fragment', 'atomization', 'fragmentation', 'atomisation']\n",
      "['sperm', 'spermatozoan', 'spermatozoon', 'sperm', 'sperm_cell']\n",
      "['nephrolithiasi', 'nephrolithiasis', 'renal_lithiasis']\n",
      "['xgboost']\n",
      "['fragment', 'atomization', 'fragmentation', 'atomisation']\n",
      "['sperm', 'spermatozoan', 'spermatozoon', 'sperm', 'sperm_cell']\n",
      "['nephrolithiasi', 'nephrolithiasis', 'renal_lithiasis']\n",
      "No match found at pdf cci.18.00080.pdf\n",
      "[False, False, False]\n",
      "['xgboost']\n",
      "['fragment', 'atomization', 'fragmentation', 'atomisation']\n",
      "['sperm', 'spermatozoan', 'spermatozoon', 'sperm', 'sperm_cell']\n",
      "['nephrolithiasi', 'nephrolithiasis', 'renal_lithiasis']\n",
      "['xgboost']\n",
      "['fragment', 'atomization', 'fragmentation', 'atomisation']\n",
      "['sperm', 'spermatozoan', 'spermatozoon', 'sperm', 'sperm_cell']\n",
      "['nephrolithiasi', 'nephrolithiasis', 'renal_lithiasis']\n",
      "['xgboost']\n",
      "['fragment', 'atomization', 'fragmentation', 'atomisation']\n",
      "['sperm', 'spermatozoan', 'spermatozoon', 'sperm', 'sperm_cell']\n",
      "['nephrolithiasi', 'nephrolithiasis', 'renal_lithiasis']\n",
      "Match found! at pdf doyle2021.pdf\n",
      "[True, True, True]\n",
      "['xgboost']\n",
      "['fragment', 'atomization', 'fragmentation', 'atomisation']\n",
      "['sperm', 'spermatozoan', 'spermatozoon', 'sperm', 'sperm_cell']\n",
      "['nephrolithiasi', 'nephrolithiasis', 'renal_lithiasis']\n",
      "['xgboost']\n",
      "['fragment', 'atomization', 'fragmentation', 'atomisation']\n",
      "['sperm', 'spermatozoan', 'spermatozoon', 'sperm', 'sperm_cell']\n",
      "['nephrolithiasi', 'nephrolithiasis', 'renal_lithiasis']\n",
      "['xgboost']\n",
      "['fragment', 'atomization', 'fragmentation', 'atomisation']\n",
      "['sperm', 'spermatozoan', 'spermatozoon', 'sperm', 'sperm_cell']\n",
      "['nephrolithiasi', 'nephrolithiasis', 'renal_lithiasis']\n",
      "No match found at pdf HBP.pdf\n",
      "[False, False, False]\n",
      "['xgboost']\n",
      "['fragment', 'atomization', 'fragmentation', 'atomisation']\n",
      "['sperm', 'spermatozoan', 'spermatozoon', 'sperm', 'sperm_cell']\n",
      "['nephrolithiasi', 'nephrolithiasis', 'renal_lithiasis']\n",
      "['xgboost']\n",
      "['fragment', 'atomization', 'fragmentation', 'atomisation']\n",
      "['sperm', 'spermatozoan', 'spermatozoon', 'sperm', 'sperm_cell']\n",
      "['nephrolithiasi', 'nephrolithiasis', 'renal_lithiasis']\n",
      "['xgboost']\n",
      "['fragment', 'atomization', 'fragmentation', 'atomisation']\n",
      "['sperm', 'spermatozoan', 'spermatozoon', 'sperm', 'sperm_cell']\n",
      "['nephrolithiasi', 'nephrolithiasis', 'renal_lithiasis']\n",
      "No match found at pdf HBP².pdf\n",
      "[False, False, False]\n",
      "['xgboost']\n",
      "['fragment', 'atomization', 'fragmentation', 'atomisation']\n",
      "['sperm', 'spermatozoan', 'spermatozoon', 'sperm', 'sperm_cell']\n",
      "['nephrolithiasi', 'nephrolithiasis', 'renal_lithiasis']\n",
      "['xgboost']\n",
      "['fragment', 'atomization', 'fragmentation', 'atomisation']\n",
      "['sperm', 'spermatozoan', 'spermatozoon', 'sperm', 'sperm_cell']\n",
      "['nephrolithiasi', 'nephrolithiasis', 'renal_lithiasis']\n",
      "['xgboost']\n",
      "['fragment', 'atomization', 'fragmentation', 'atomisation']\n",
      "['sperm', 'spermatozoan', 'spermatozoon', 'sperm', 'sperm_cell']\n",
      "['nephrolithiasi', 'nephrolithiasis', 'renal_lithiasis']\n",
      "No match found at pdf icu-61-239.pdf\n",
      "[False, False, False]\n",
      "['xgboost']\n",
      "['fragment', 'atomization', 'fragmentation', 'atomisation']\n",
      "['sperm', 'spermatozoan', 'spermatozoon', 'sperm', 'sperm_cell']\n",
      "['nephrolithiasi', 'nephrolithiasis', 'renal_lithiasis']\n",
      "['xgboost']\n",
      "['fragment', 'atomization', 'fragmentation', 'atomisation']\n",
      "['sperm', 'spermatozoan', 'spermatozoon', 'sperm', 'sperm_cell']\n",
      "['nephrolithiasi', 'nephrolithiasis', 'renal_lithiasis']\n",
      "['xgboost']\n",
      "['fragment', 'atomization', 'fragmentation', 'atomisation']\n",
      "['sperm', 'spermatozoan', 'spermatozoon', 'sperm', 'sperm_cell']\n",
      "['nephrolithiasi', 'nephrolithiasis', 'renal_lithiasis']\n",
      "Match found! at pdf reproductive AI.pdf\n",
      "[True, True, True]\n",
      "['xgboost']\n",
      "['fragment', 'atomization', 'fragmentation', 'atomisation']\n",
      "['sperm', 'spermatozoan', 'spermatozoon', 'sperm', 'sperm_cell']\n",
      "['nephrolithiasi', 'nephrolithiasis', 'renal_lithiasis']\n",
      "['xgboost']\n",
      "['fragment', 'atomization', 'fragmentation', 'atomisation']\n",
      "['sperm', 'spermatozoan', 'spermatozoon', 'sperm', 'sperm_cell']\n",
      "['nephrolithiasi', 'nephrolithiasis', 'renal_lithiasis']\n",
      "['xgboost']\n",
      "['fragment', 'atomization', 'fragmentation', 'atomisation']\n",
      "['sperm', 'spermatozoan', 'spermatozoon', 'sperm', 'sperm_cell']\n",
      "['nephrolithiasi', 'nephrolithiasis', 'renal_lithiasis']\n",
      "Match found! at pdf s12911-021-01585-9.pdf\n",
      "[True, True, True]\n",
      "['xgboost']\n",
      "['fragment', 'atomization', 'fragmentation', 'atomisation']\n",
      "['sperm', 'spermatozoan', 'spermatozoon', 'sperm', 'sperm_cell']\n",
      "['nephrolithiasi', 'nephrolithiasis', 'renal_lithiasis']\n",
      "['xgboost']\n",
      "['fragment', 'atomization', 'fragmentation', 'atomisation']\n",
      "['sperm', 'spermatozoan', 'spermatozoon', 'sperm', 'sperm_cell']\n",
      "['nephrolithiasi', 'nephrolithiasis', 'renal_lithiasis']\n",
      "['xgboost']\n",
      "['fragment', 'atomization', 'fragmentation', 'atomisation']\n",
      "['sperm', 'spermatozoan', 'spermatozoon', 'sperm', 'sperm_cell']\n",
      "['nephrolithiasi', 'nephrolithiasis', 'renal_lithiasis']\n",
      "No match found at pdf s41585-020-0312-1.pdf\n",
      "[False, False, False]\n",
      "['xgboost']\n",
      "['fragment', 'atomization', 'fragmentation', 'atomisation']\n",
      "['sperm', 'spermatozoan', 'spermatozoon', 'sperm', 'sperm_cell']\n",
      "['nephrolithiasi', 'nephrolithiasis', 'renal_lithiasis']\n",
      "['xgboost']\n",
      "['fragment', 'atomization', 'fragmentation', 'atomisation']\n",
      "['sperm', 'spermatozoan', 'spermatozoon', 'sperm', 'sperm_cell']\n",
      "['nephrolithiasi', 'nephrolithiasis', 'renal_lithiasis']\n",
      "['xgboost']\n",
      "['fragment', 'atomization', 'fragmentation', 'atomisation']\n",
      "['sperm', 'spermatozoan', 'spermatozoon', 'sperm', 'sperm_cell']\n",
      "['nephrolithiasi', 'nephrolithiasis', 'renal_lithiasis']\n",
      "No match found at pdf teichmann2007.pdf\n",
      "[False, False, False]\n",
      "['xgboost']\n",
      "['fragment', 'atomization', 'fragmentation', 'atomisation']\n",
      "['sperm', 'spermatozoan', 'spermatozoon', 'sperm', 'sperm_cell']\n",
      "['nephrolithiasi', 'nephrolithiasis', 'renal_lithiasis']\n",
      "['xgboost']\n",
      "['fragment', 'atomization', 'fragmentation', 'atomisation']\n",
      "['sperm', 'spermatozoan', 'spermatozoon', 'sperm', 'sperm_cell']\n",
      "['nephrolithiasi', 'nephrolithiasis', 'renal_lithiasis']\n",
      "['xgboost']\n",
      "['fragment', 'atomization', 'fragmentation', 'atomisation']\n",
      "['sperm', 'spermatozoan', 'spermatozoon', 'sperm', 'sperm_cell']\n",
      "['nephrolithiasi', 'nephrolithiasis', 'renal_lithiasis']\n",
      "No match found at pdf tju-46-supplement1-s27.pdf\n",
      "[False, False, False]\n",
      "['xgboost']\n",
      "['fragment', 'atomization', 'fragmentation', 'atomisation']\n",
      "['sperm', 'spermatozoan', 'spermatozoon', 'sperm', 'sperm_cell']\n",
      "['nephrolithiasi', 'nephrolithiasis', 'renal_lithiasis']\n",
      "['xgboost']\n",
      "['fragment', 'atomization', 'fragmentation', 'atomisation']\n",
      "['sperm', 'spermatozoan', 'spermatozoon', 'sperm', 'sperm_cell']\n",
      "['nephrolithiasi', 'nephrolithiasis', 'renal_lithiasis']\n",
      "['xgboost']\n",
      "['fragment', 'atomization', 'fragmentation', 'atomisation']\n",
      "['sperm', 'spermatozoan', 'spermatozoon', 'sperm', 'sperm_cell']\n",
      "['nephrolithiasi', 'nephrolithiasis', 'renal_lithiasis']\n",
      "No match found at pdf urolithiasis and AI.pdf\n",
      "[False, False, False]\n",
      "['xgboost']\n",
      "['fragment', 'atomization', 'fragmentation', 'atomisation']\n",
      "['sperm', 'spermatozoan', 'spermatozoon', 'sperm', 'sperm_cell']\n",
      "['nephrolithiasi', 'nephrolithiasis', 'renal_lithiasis']\n",
      "['xgboost']\n",
      "['fragment', 'atomization', 'fragmentation', 'atomisation']\n",
      "['sperm', 'spermatozoan', 'spermatozoon', 'sperm', 'sperm_cell']\n",
      "['nephrolithiasi', 'nephrolithiasis', 'renal_lithiasis']\n",
      "['xgboost']\n",
      "['fragment', 'atomization', 'fragmentation', 'atomisation']\n",
      "['sperm', 'spermatozoan', 'spermatozoon', 'sperm', 'sperm_cell']\n",
      "['nephrolithiasi', 'nephrolithiasis', 'renal_lithiasis']\n",
      "No match found at pdf wollin1998 (1).pdf\n",
      "[False, False, False]\n",
      "['xgboost']\n",
      "['fragment', 'atomization', 'fragmentation', 'atomisation']\n",
      "['sperm', 'spermatozoan', 'spermatozoon', 'sperm', 'sperm_cell']\n",
      "['nephrolithiasi', 'nephrolithiasis', 'renal_lithiasis']\n",
      "['xgboost']\n",
      "['fragment', 'atomization', 'fragmentation', 'atomisation']\n",
      "['sperm', 'spermatozoan', 'spermatozoon', 'sperm', 'sperm_cell']\n",
      "['nephrolithiasi', 'nephrolithiasis', 'renal_lithiasis']\n",
      "['xgboost']\n",
      "['fragment', 'atomization', 'fragmentation', 'atomisation']\n",
      "['sperm', 'spermatozoan', 'spermatozoon', 'sperm', 'sperm_cell']\n",
      "['nephrolithiasi', 'nephrolithiasis', 'renal_lithiasis']\n",
      "No match found at pdf wollin1998.pdf\n",
      "[False, False, False]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "query = \"(xgboost or fragmentation) and sperm and nephrolithiasis \"\n",
    "\n",
    "# define a dictionary of synonyms\n",
    "synonyms = {}\n",
    "for term in re.findall(r'\\b\\w+\\b', query):\n",
    "    synonyms[term] = set()\n",
    "    for syn in wordnet.synsets(term):\n",
    "        for lemma in syn.lemmas():\n",
    "            synonyms[term].add(lemma.name())\n",
    "\n",
    "or_keywords = [k.strip() for k in query.split(\"and\")]\n",
    "and_keywords = []\n",
    "for match in re.findall(r'\\((.*?)\\)|(\\b\\w+\\b)|(\\b(or)\\b)|(\\b(and)\\b)', query):\n",
    "    keyword = \"\".join([x for x in match if x]).strip().lower()\n",
    "    if keyword:\n",
    "        if keyword != \"or\" and \"or\" in keyword:\n",
    "            and_keywords.append([[PorterStemmer().stem(w.strip().lower())]+list(synonyms.get(w.strip().lower(), set())) for w in keyword.split(\"or\")])\n",
    "        elif keyword != \"and\" and \"and\" in keyword:\n",
    "            and_keywords.append([[PorterStemmer().stem(w.strip().lower())]+list(synonyms.get(w.strip().lower(), set())) for w in keyword.split(\"and\")])\n",
    "        else:\n",
    "            and_keywords.append([[PorterStemmer().stem(keyword)]+list(synonyms.get(keyword, set()))])\n",
    "\n",
    "and_keywords = [x for x in and_keywords if x != [['or']] and not any('or' in sublist for sublist in x) and x != [['and']] and not any('and' in sublist for sublist in x)]\n",
    "\n",
    "print(and_keywords)\n",
    "print(or_keywords)\n",
    "for i in range(len(data[\"text\"])):\n",
    "    matches = []\n",
    "    for or_keywords_group in or_keywords:\n",
    "        or_matches = []\n",
    "        for and_keywords_group in and_keywords:\n",
    "            and_matches = []\n",
    "            for keyword_set in and_keywords_group:\n",
    "                print(keyword_set)\n",
    "                found = any([str(t) in PorterStemmer().stem(data[\"text\"][i].lower()) for t in keyword_set])\n",
    "                and_matches.append(found)\n",
    "            or_matches.append(all(and_matches))\n",
    "        matches.append(any(or_matches))\n",
    "\n",
    "    if all(matches):\n",
    "        print(\"Match found! at pdf\", data[\"titre\"][i])\n",
    "        print(matches)\n",
    "    else:\n",
    "        print(\"No match found at pdf\", data[\"titre\"][i])\n",
    "        print(matches)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No match found at pdf 1-s2.0-S0090429518305971-main.pdf\n",
      "[False]\n",
      "No match found at pdf 1-s2.0-S0302283818307401-main.pdf\n",
      "[False]\n",
      "Match found! at pdf 1-s2.0-S258893112300038X-main.pdf\n",
      "[True]\n",
      "No match found at pdf 10.1007@s00345-019-03000-5.pdf\n",
      "[False]\n",
      "No match found at pdf abbod2007.pdf\n",
      "[False]\n",
      "No match found at pdf AI and reproductive.pdf\n",
      "[False]\n",
      "No match found at pdf Cancer Science - 2022 - Iwamura - Machine learning diagnosis by immunoglobulin N‐glycan signatures for precision diagnosis.pdf\n",
      "[False]\n",
      "No match found at pdf cci.18.00080.pdf\n",
      "[False]\n",
      "No match found at pdf doyle2021.pdf\n",
      "[False]\n",
      "No match found at pdf HBP.pdf\n",
      "[False]\n",
      "No match found at pdf HBP².pdf\n",
      "[False]\n",
      "No match found at pdf icu-61-239.pdf\n",
      "[False]\n",
      "Match found! at pdf reproductive AI.pdf\n",
      "[True]\n",
      "No match found at pdf s12911-021-01585-9.pdf\n",
      "[False]\n",
      "No match found at pdf s41585-020-0312-1.pdf\n",
      "[False]\n",
      "No match found at pdf teichmann2007.pdf\n",
      "[False]\n",
      "No match found at pdf tju-46-supplement1-s27.pdf\n",
      "[False]\n",
      "No match found at pdf urolithiasis and AI.pdf\n",
      "[False]\n",
      "No match found at pdf wollin1998 (1).pdf\n",
      "[False]\n",
      "No match found at pdf wollin1998.pdf\n",
      "[False]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "query = \" xgboost or urology\"\n",
    "# define a dictionary of synonyms\n",
    "synonyms = {}\n",
    "for term in re.findall(r'\\b\\w+\\b', query):\n",
    "    synonyms[term] = set()\n",
    "    for syn in wordnet.synsets(term):\n",
    "        for lemma in syn.lemmas():\n",
    "            synonyms[term].add(lemma.name())\n",
    "\n",
    "or_keywords = [k.strip() for k in query.split(\"and\")]\n",
    "and_keywords = []\n",
    "for match in re.findall(r'\\((.*?)\\)|(\\b\\w+\\b)|(\\b(or)\\b)|(\\b(and)\\b)', query):\n",
    "    keyword = \"\".join([x for x in match if x]).strip().lower()\n",
    "    if keyword:\n",
    "        if keyword != \"or\" and \"or\" in keyword:\n",
    "            and_keywords.append([[PorterStemmer().stem(w.strip().lower())]+list(synonyms.get(w.strip().lower(), set())) for w in keyword.split(\"or\")])\n",
    "        elif keyword != \"and\" and \"and\" in keyword:\n",
    "            and_keywords.append([[PorterStemmer().stem(w.strip().lower())]+list(synonyms.get(w.strip().lower(), set())) for w in keyword.split(\"and\")])\n",
    "        else:\n",
    "            and_keywords.append([[PorterStemmer().stem(keyword)]+list(synonyms.get(keyword, set()))])\n",
    "\n",
    "and_keywords = [x for x in and_keywords if x != [['or']] and not any('or' in sublist for sublist in x) and x != [['and']] and not any('and' in sublist for sublist in x)]\n",
    "for i in range(len(data[\"text\"])):\n",
    "    matches = []\n",
    "    if len(or_keywords) == 1:\n",
    "        k = or_keywords[0]\n",
    "        or_matches = []\n",
    "        for w in k.split(\"or\"):\n",
    "            found = any([t in str(PorterStemmer().stem(data[\"text\"][i].lower())) for t in and_keywords[0][0]])\n",
    "            or_matches.append(found)\n",
    "        matches.append(any(or_matches))\n",
    "    elif len(and_keywords) == 1:\n",
    "        k = and_keywords[0]\n",
    "        and_matches = []\n",
    "        for w in k:\n",
    "            found = any([t in str(PorterStemmer().stem(data[\"text\"][i].lower())) for t in w])\n",
    "            and_matches.append(found)\n",
    "        matches.append(all(and_matches))\n",
    "    elif len(and_keywords) == 0:  # Single word query\n",
    "        word = or_keywords[0]\n",
    "        matches.append(word in str(PorterStemmer().stem(data[\"text\"][i].lower())))\n",
    "    else:\n",
    "        for k in and_keywords:\n",
    "            or_matches = []\n",
    "            for w in k:\n",
    "                found = any([t in str(PorterStemmer().stem(data[\"text\"][i].lower())) for t in w])\n",
    "                or_matches.append(found)\n",
    "            matches.append(any(or_matches))\n",
    "        # matches = [m for m in matches if m]\n",
    " \n",
    "    if all(matches):\n",
    "        print(\"Match found! at pdf\",data[\"titre\"][i])\n",
    "    else:\n",
    "        print(\"No match found at pdf\",data[\"titre\"][i])\n",
    "    print(matches)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(data[\"text\"])):\n",
    "    matches = []\n",
    "    if len(or_keywords) == 1:\n",
    "        k = or_keywords[0]\n",
    "        or_matches = []\n",
    "        for w in k.split(\"or\"):\n",
    "            found = any([PorterStemmer().stem(w.lower()) == str(t) for t in and_keywords[0]])\n",
    "            or_matches.append(found)\n",
    "        matches.append(any(or_matches))\n",
    "    elif len(and_keywords) == 1:\n",
    "        k = and_keywords[0]\n",
    "        and_matches = []\n",
    "        for w in k:\n",
    "            found = any([PorterStemmer().stem(w.lower()) == str(t) for t in w])\n",
    "            and_matches.append(found)\n",
    "        matches.append(all(and_matches))\n",
    "    else:\n",
    "        for k in and_keywords:\n",
    "            or_matches = []\n",
    "            for w in k:\n",
    "                found = any([PorterStemmer().stem(w.lower()) == str(t) for t in w])\n",
    "                or_matches.append(found)\n",
    "            matches.append(any(or_matches))\n",
    " \n",
    "    if all(matches):\n",
    "        print(\"Match found! at pdf\", data[\"titre\"][i])\n",
    "    else:\n",
    "        print(\"No match found at pdf\", data[\"titre\"][i])\n",
    "    print(matches)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['xgboost']],\n",
       " [['or',\n",
       "   'operating_theatre',\n",
       "   'operating_room',\n",
       "   'Oregon',\n",
       "   'surgery',\n",
       "   'OR',\n",
       "   'Beaver_State',\n",
       "   'operating_theater']],\n",
       " [['fragment', 'atomization', 'fragmentation', 'atomisation']]]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"xgboost or fragmentation \"\n",
    "\n",
    "# define a dictionary of synonyms\n",
    "synonyms = {}\n",
    "for term in re.findall(r'\\b\\w+\\b', query):\n",
    "    synonyms[term] = set()\n",
    "    for syn in wordnet.synsets(term):\n",
    "        for lemma in syn.lemmas():\n",
    "            synonyms[term].add(lemma.name())\n",
    "\n",
    "or_keywords = [k.strip() for k in query.split(\"and\")]\n",
    "and_keywords = []\n",
    "for match in re.findall(r'\\((.*?)\\)|(\\b\\w+\\b)|(\\b(or)\\b)|(\\b(and)\\b)', query):\n",
    "    keyword = \"\".join([x for x in match if x]).strip().lower()\n",
    "    if keyword:\n",
    "        if keyword != \"or\" and \"or\" in keyword:\n",
    "            and_keywords.append([[PorterStemmer().stem(w.strip().lower())]+list(synonyms.get(w.strip().lower(), set())) for w in keyword.split(\"or\")])\n",
    "        elif keyword != \"and\" and \"and\" in keyword:\n",
    "            and_keywords.append([[PorterStemmer().stem(w.strip().lower())]+list(synonyms.get(w.strip().lower(), set())) for w in keyword.split(\"and\")])\n",
    "        else:\n",
    "            and_keywords.append([[PorterStemmer().stem(keyword)]+list(synonyms.get(keyword, set()))])\n",
    "\n",
    "and_keywords = [x for x in and_keywords if  x != [['or']] and x!=[['and']]]\n",
    "and_keywords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sperm appears 0 times in cluster_0\n",
      "sperm appears 0 times in cluster_1\n",
      "sperm appears 0 times in cluster_2\n",
      "sperm appears 0 times in cluster_3\n",
      "sperm appears 0 times in cluster_4\n",
      "sperm appears 0 times in cluster_5\n",
      "sperm appears 1 times in cluster_6\n",
      "sperm appears 0 times in cluster_7\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# search for 1 word\n",
    "search_word = \"sperm\"\n",
    "# Using loops\n",
    "counter = {key: 0 for key in cluster.keys()}\n",
    "\n",
    "# Loop over each cluster and count occurrences of search_word\n",
    "for key, value in cluster.items():\n",
    "    for word in value:\n",
    "        if str(word) == str(search_word):\n",
    "            counter[key] += 1\n",
    "\n",
    "# Print the counter for each cluster\n",
    "for key, value in counter.items():\n",
    "    print(f\"{search_word} appears {value} times in cluster_{key}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search for 1 word\n",
    "def search_one_word_cluster(word):\n",
    "    search_word_list=tokenize_and_stem(word)\n",
    "    # loop over each word serach\n",
    "    for word_ in search_word_list:\n",
    "        # Using loops\n",
    "        counter = {key: 0 for key in cluster.keys()}\n",
    "\n",
    "        # Loop over each cluster and count occurrences of search_word\n",
    "        for key, value in cluster.items():\n",
    "            for word in value:\n",
    "                if str(word) == str(word_):\n",
    "                    counter[key] += 1\n",
    "\n",
    "        # Print the counter for each cluster\n",
    "        for key, value in counter.items():\n",
    "            print(f\"{word_} appears {value} times in cluster_{key}\")\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search for 2 word\n",
    "def search_words_cluster(text):\n",
    "    \n",
    "    search_word_list=tokenize_and_stem(text)\n",
    "    #for i in range(len(search_word_list)):\n",
    "        #search_word_list[i]=search_word_list[i].strip()# supprimer les escpaces\n",
    "    # loop over each word serach\n",
    "    for word_ in search_word_list:\n",
    "        # Using loops\n",
    "        counter = {key: 0 for key in cluster.keys()}\n",
    "\n",
    "        # Loop over each cluster and count occurrences of search_word\n",
    "        for key, value in cluster.items():\n",
    "            for word in value:\n",
    "                if str(word) == str(word_):\n",
    "                    counter[key] += 1\n",
    "\n",
    "        # Print the counter for each cluster\n",
    "        for key, value in counter.items():\n",
    "            print(f\"{word_} appears {value} times in cluster_{key}\")\n",
    "        print(\"/////////////////////////////\")        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sperm appears 0 times in cluster_0\n",
      "sperm appears 0 times in cluster_1\n",
      "sperm appears 0 times in cluster_2\n",
      "sperm appears 0 times in cluster_3\n",
      "sperm appears 0 times in cluster_4\n",
      "sperm appears 0 times in cluster_5\n",
      "sperm appears 1 times in cluster_6\n",
      "sperm appears 0 times in cluster_7\n"
     ]
    }
   ],
   "source": [
    "search_one_word_cluster(\"sperm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Homogeneity: 0.612\n",
      "Completeness: 1.000\n",
      "V-measure: 0.759\n",
      "Adjusted Rand-Index: 0.047\n",
      "Silhouette Coefficient: 0.164\n"
     ]
    }
   ],
   "source": [
    "#evaluators\n",
    "print(\"Homogeneity: %0.3f\" % metrics.homogeneity_score(data[\"text\"], data['Cluster']))\n",
    "print(\"Completeness: %0.3f\" % metrics.completeness_score(data[\"text\"], data['Cluster']))\n",
    "print(\"V-measure: %0.3f\" % metrics.v_measure_score(data[\"text\"], data[\"Cluster\"]))\n",
    "print(\"Adjusted Rand-Index: %.3f\" % metrics.adjusted_rand_score(data[\"text\"], data[\"Cluster\"]))\n",
    "print(\"Silhouette Coefficient: %0.3f\" % metrics.silhouette_score(tfidf_matrix,data[\"Cluster\"] , sample_size=1000))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.280167055464391e-78\n",
      "1.1036040690772268e-77\n",
      "1.1036040690772268e-77\n",
      "1.1036040690772268e-77\n",
      "8.636168555094496e-78\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ma.jari\\.conda\\envs\\salle_IA\\lib\\site-packages\\nltk\\translate\\bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'tokenize_text'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\ma.jari\\.conda\\envs\\salle_IA\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3802\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3801\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 3802\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n\u001b[0;32m   3803\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[1;32mc:\\Users\\ma.jari\\.conda\\envs\\salle_IA\\lib\\site-packages\\pandas\\_libs\\index.pyx:138\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\ma.jari\\.conda\\envs\\salle_IA\\lib\\site-packages\\pandas\\_libs\\index.pyx:165\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5745\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5753\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'tokenize_text'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m10\u001b[39m):\n\u001b[1;32m----> 8\u001b[0m     score \u001b[38;5;241m=\u001b[39m sentence_bleu(\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtokenize_text\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m[i],candidate,weights\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m0.25\u001b[39m,\u001b[38;5;241m0.25\u001b[39m,\u001b[38;5;241m0.25\u001b[39m,\u001b[38;5;241m0.25\u001b[39m))\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28mprint\u001b[39m(score)\n",
      "File \u001b[1;32mc:\\Users\\ma.jari\\.conda\\envs\\salle_IA\\lib\\site-packages\\pandas\\core\\frame.py:3807\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3805\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mnlevels \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m   3806\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3807\u001b[0m indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns\u001b[39m.\u001b[39;49mget_loc(key)\n\u001b[0;32m   3808\u001b[0m \u001b[39mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   3809\u001b[0m     indexer \u001b[39m=\u001b[39m [indexer]\n",
      "File \u001b[1;32mc:\\Users\\ma.jari\\.conda\\envs\\salle_IA\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3804\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3802\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine\u001b[39m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3803\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m-> 3804\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[0;32m   3805\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[0;32m   3806\u001b[0m     \u001b[39m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3807\u001b[0m     \u001b[39m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3808\u001b[0m     \u001b[39m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3809\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'tokenize_text'"
     ]
    }
   ],
   "source": [
    "\n",
    "candidate = \"sperm\"\n",
    "for i in range(0,5):\n",
    "    score = sentence_bleu(cluster[i], candidate, weights=(0.25, 0.25, 0.25, 0.25))\n",
    "    \n",
    "    print(score)\n",
    "print(\"\\n\")\n",
    "for i in range(0,10):\n",
    "    score = sentence_bleu(data[\"tokenize_text\"][i],candidate,weights=(0.25,0.25,0.25,0.25))\n",
    "    print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def tf_idf_similarity(text,data):\n",
    "# Recherche d'articles similaires\n",
    "    query = text\n",
    "    query_tfidf = tfidf_vectorizer.transform([query])\n",
    "    similarity_scores = cosine_similarity(query_tfidf, tfidf_matrix)\n",
    "\n",
    "    # Classement des articles par ordre décroissant de similarité\n",
    "    similarity_scores = similarity_scores[0][np.argsort(data.index)]\n",
    "    data['similarity'] = list(similarity_scores)\n",
    "    # Sort the dataframe by document ID or title\n",
    "    data = data.sort_values('titre')\n",
    "\n",
    "    # Reset the index to ensure that the order of the documents is always the same\n",
    "    data = data.reset_index(drop=True)\n",
    "    index_max=int(np.argmax(data[\"similarity\"]))\n",
    "    cluster_index_max=data[\"Cluster\"][index_max]\n",
    "    # Affichage des articles similaires\n",
    "    print(data.loc[data[\"Cluster\"]==cluster_index_max,[\"titre\",\"similarity\",\"Cluster\"]].sort_values([\"similarity\"],ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                titre  similarity  Cluster\n",
      "0   1-s2.0-S0090429518305971-main.pdf         0.0        1\n",
      "3      10.1007@s00345-019-03000-5.pdf         0.0        1\n",
      "8                       abbod2007.pdf         0.0        1\n",
      "10                      doyle2021.pdf         0.0        1\n",
      "11                     icu-61-239.pdf         0.0        1\n",
      "13             s12911-021-01585-9.pdf         0.0        1\n",
      "16         tju-46-supplement1-s27.pdf         0.0        1\n",
      "17            urolithiasis and AI.pdf         0.0        1\n"
     ]
    }
   ],
   "source": [
    "tf_idf_similarity(\"aefe\",data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Match found! at pdf 1-s2.0-S0090429518305971-main.pdf\n",
      "Match found! at pdf 1-s2.0-S0302283818307401-main.pdf\n",
      "Match found! at pdf 1-s2.0-S258893112300038X-main.pdf\n",
      "Match found! at pdf 10.1007@s00345-019-03000-5.pdf\n",
      "Match found! at pdf abbod2007.pdf\n",
      "Match found! at pdf AI and reproductive.pdf\n",
      "Match found! at pdf Cancer Science - 2022 - Iwamura - Machine learning diagnosis by immunoglobulin N‐glycan signatures for precision diagnosis.pdf\n",
      "Match found! at pdf cci.18.00080.pdf\n",
      "Match found! at pdf doyle2021.pdf\n",
      "Match found! at pdf HBP.pdf\n",
      "Match found! at pdf HBP².pdf\n",
      "Match found! at pdf icu-61-239.pdf\n",
      "Match found! at pdf reproductive AI.pdf\n",
      "Match found! at pdf s12911-021-01585-9.pdf\n",
      "Match found! at pdf s41585-020-0312-1.pdf\n",
      "Match found! at pdf teichmann2007.pdf\n",
      "Match found! at pdf tju-46-supplement1-s27.pdf\n",
      "Match found! at pdf urolithiasis and AI.pdf\n",
      "Match found! at pdf wollin1998 (1).pdf\n",
      "Match found! at pdf wollin1998.pdf\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Exemple de requête\n",
    "query = \"(xgboost or surgic) and sperm\"\n",
    "\n",
    "# Identifier les groupes de mots clés associés à chaque opérateur booléen\n",
    "or_groups = re.findall(r'\\([^()]* or [^()]*\\)', query)\n",
    "and_groups = re.findall(r'and', query)\n",
    "\n",
    "# Rechercher chaque groupe de mots clés, en utilisant la logique booléenne pour combiner les résultats\n",
    "for i in range(len(data[\"text\"])):\n",
    "    matches = []\n",
    "    for group in or_groups:\n",
    "        keywords = [k.strip().lower() for k in group[1:-1].split(\"or\")]\n",
    "        found = any(all(word.strip().lower() in str(data[\"text\"][i]).lower() for word in keywords) for i in range(len(data[\"text\"])))\n",
    "        matches.append(found)\n",
    "        \n",
    "    for group in and_groups:\n",
    "        keywords = [k.strip().lower() for k in group[1:-1].split(\"and\")]\n",
    "        found = all(any(word.strip().lower() in str(data[\"text\"][i]).lower() for word in keywords) for i in range(len(data[\"text\"])))\n",
    "        matches.append(found)\n",
    "        \n",
    "    if any(matches):\n",
    "            print(\"Match found! at pdf\",data[\"titre\"][i])\n",
    "    else:\n",
    "            print(\"No match found at pdf\",data[\"titre\"][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Match found! at pdf  1-s2.0-S0090429518305971-main.pdf\n",
      "Match found! at pdf  1-s2.0-S0302283818307401-main.pdf\n",
      "Match found! at pdf  1-s2.0-S258893112300038X-main.pdf\n",
      "Match found! at pdf  10.1007@s00345-019-03000-5.pdf\n",
      "No match found at abbod2007.pdf\n",
      "Match found! at pdf  AI and reproductive.pdf\n",
      "No match found at Cancer Science - 2022 - Iwamura - Machine learning diagnosis by immunoglobulin N‐glycan signatures for precision diagnosis.pdf\n",
      "Match found! at pdf  cci.18.00080.pdf\n",
      "Match found! at pdf  doyle2021.pdf\n",
      "Match found! at pdf  HBP.pdf\n",
      "No match found at HBP².pdf\n",
      "No match found at icu-61-239.pdf\n",
      "Match found! at pdf  reproductive AI.pdf\n",
      "Match found! at pdf  s12911-021-01585-9.pdf\n",
      "Match found! at pdf  s41585-020-0312-1.pdf\n",
      "Match found! at pdf  teichmann2007.pdf\n",
      "Match found! at pdf  tju-46-supplement1-s27.pdf\n",
      "Match found! at pdf  urolithiasis and AI.pdf\n",
      "No match found at wollin1998 (1).pdf\n",
      "No match found at wollin1998.pdf\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "query = \"(xgboost or surgic )and sperm\"\n",
    "\n",
    "# extract keywords with parentheses\n",
    "keywords = re.findall(r'\\((.*?)\\)|\\b\\w+\\b', query)\n",
    "keywords = [k.strip().lower() for k in keywords if k.strip()]\n",
    "\n",
    "# handle OR and AND operators\n",
    "i = 0\n",
    "while i < len(keywords):\n",
    "    if keywords[i] == \"or\":\n",
    "        keywords[i-1:i+2] = [keywords[i-1] + \" or \" + keywords[i+1]]\n",
    "    elif keywords[i] == \"and\":\n",
    "        keywords[i-1:i+2] = [keywords[i-1] + \" and \" + keywords[i+1]]\n",
    "    else:\n",
    "        i += 1\n",
    "\n",
    "# search for matches\n",
    "for i in range(len(data[\"text\"])):\n",
    "    matches = []\n",
    "    for k in keywords:\n",
    "        if \"and\" in k:\n",
    "            and_keywords = k.split(\"and\")\n",
    "            found = all(word.strip().lower() in str(data[\"text\"][i]).lower() for word in and_keywords)\n",
    "            matches.append(found)\n",
    "        elif \"or\" in k:\n",
    "            or_keywords = k.split(\"or\")\n",
    "            found = any(word.strip().lower() in str(data[\"text\"][i]).lower() for word in or_keywords)\n",
    "            matches.append(found)\n",
    "        else:\n",
    "            found = k.strip() in str(data[\"text\"][i]).lower()\n",
    "            matches.append(found)\n",
    "            \n",
    "    if any(matches):\n",
    "        print(\"Match found! at pdf \",data[\"titre\"][i])\n",
    "    else:\n",
    "        print(\"No match found at\",data[\"titre\"][i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "query = \"xgboost\"\n",
    "result = []\n",
    "# define a dictionary of synonyms\n",
    "synonyms = {}\n",
    "for term in re.findall(r'\\b\\w+\\b', query):\n",
    "    synonyms[term] = set()\n",
    "    for syn in wordnet.synsets(term):\n",
    "        for lemma in syn.lemmas():\n",
    "            synonyms[term].add(lemma.name())\n",
    "\n",
    "or_keywords = [k.strip() for k in query.split(\"and\")]\n",
    "and_keywords = []\n",
    "for match in re.findall(r'\\((.*?)\\)|(\\b\\w+\\b)|(\\b(or)\\b)|(\\b(and)\\b)', query):\n",
    "    keyword = \"\".join([x for x in match if x]).strip().lower()\n",
    "    if keyword:\n",
    "        if keyword != \"or\" and \"or\" in keyword:\n",
    "            and_keywords.append([[PorterStemmer().stem(w.strip().lower())]+list(synonyms.get(w.strip().lower(), set())) for w in keyword.split(\"or\")])\n",
    "        elif keyword != \"and\" and \"and\" in keyword:\n",
    "            and_keywords.append([[PorterStemmer().stem(w.strip().lower())]+list(synonyms.get(w.strip().lower(), set())) for w in keyword.split(\"and\")])\n",
    "        else:\n",
    "            and_keywords.append([[PorterStemmer().stem(keyword)]+list(synonyms.get(keyword, set()))])\n",
    "\n",
    "and_keywords = [x for x in and_keywords if  x != [['or']] and x!=[['and']]]\n",
    "\n",
    "for i in range(len(data[\"text\"])):\n",
    "    matches = []\n",
    "    if len(or_keywords) == 1:\n",
    "        k = or_keywords[0]\n",
    "        or_matches = []\n",
    "        for w in k.split(\"or\"):\n",
    "            stemmed_text = [PorterStemmer().stem(word.lower()) for word in data[\"text\"][i].split()]\n",
    "            found = any([t in stemmed_text for t in and_keywords[0]])\n",
    "            or_matches.append(found)\n",
    "        matches.append(any(or_matches))\n",
    "    elif len(and_keywords) == 1:\n",
    "        k = and_keywords[0]\n",
    "        and_matches = []\n",
    "        for w in k:\n",
    "            found = any([t in str(PorterStemmer().stem(data[\"text\"][i].lower())) for t in w])\n",
    "            and_matches.append(found)\n",
    "        matches.append(all(and_matches))\n",
    "    else:\n",
    "        for k in and_keywords:\n",
    "            or_matches = []\n",
    "            for w in k:\n",
    "                found = any([t in str(PorterStemmer().stem(data[\"text\"][i].lower())) for t in w])\n",
    "                or_matches.append(found)\n",
    "            matches.append(any(or_matches))\n",
    "        #matches = [m for m in matches if m]\n",
    " \n",
    "    if all(matches):\n",
    "       result.append(data[\"titre\"][i])\n",
    "    \n",
    "        \n",
    "    \"\"\" else:\n",
    "        print(\"No match found at pdf\",data[\"titre\"][i])\n",
    "    print(matches) \"\"\"\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "query = \"sperm\"\n",
    "\n",
    "# define a dictionary of synonyms\n",
    "synonyms = {}\n",
    "for term in re.findall(r'\\b\\w+\\b', query):\n",
    "    synonyms[term] = set()\n",
    "    for syn in wordnet.synsets(term):\n",
    "        for lemma in syn.lemmas():\n",
    "            synonyms[term].add(lemma.name())\n",
    "\n",
    "or_keywords = [k.strip() for k in query.split(\"and\")]\n",
    "and_keywords = []\n",
    "for match in re.findall(r'\\((.*?)\\)|(\\b\\w+\\b)|(\\b(or)\\b)|(\\b(and)\\b)', query):\n",
    "    keyword = \"\".join([x for x in match if x]).strip().lower()\n",
    "    if keyword:\n",
    "        if keyword != \"or\" and \"or\" in keyword:\n",
    "            and_keywords.append([[PorterStemmer().stem(w.strip().lower())]+list(synonyms.get(w.strip().lower(), set())) for w in keyword.split(\"or\")])\n",
    "        elif keyword != \"and\" and \"and\" in keyword:\n",
    "            and_keywords.append([[PorterStemmer().stem(w.strip().lower())]+list(synonyms.get(w.strip().lower(), set())) for w in keyword.split(\"and\")])\n",
    "        else:\n",
    "            and_keywords.append([[PorterStemmer().stem(keyword)]+list(synonyms.get(keyword, set()))])\n",
    "\n",
    "and_keywords = [x for x in and_keywords if  x != [['or']] and x!=[['and']]]\n",
    "\n",
    "for i in range(len(data[\"text\"])):\n",
    "    matches = []\n",
    "    if len(or_keywords) == 1:\n",
    "        k = or_keywords[0]\n",
    "        or_matches = []\n",
    "        for w in k.split(\"or\"):\n",
    "            stemmed_text = [PorterStemmer().stem(word.lower()) for word in data[\"text\"][i].split()]\n",
    "            found = any([t in stemmed_text for t in and_keywords[0]])\n",
    "            or_matches.append(found)\n",
    "        matches.append(any(or_matches))\n",
    "    elif len(and_keywords) == 1:\n",
    "        k = and_keywords[0]\n",
    "        and_matches = []\n",
    "        for w in k:\n",
    "            found = any([t in str(PorterStemmer().stem(data[\"text\"][i].lower())) for t in w])\n",
    "            and_matches.append(found)\n",
    "        matches.append(all(and_matches))\n",
    "    else:\n",
    "        for k in and_keywords:\n",
    "            or_matches = []\n",
    "            for w in k:\n",
    "                found = any([t in str(PorterStemmer().stem(data[\"text\"][i].lower())) for t in w])\n",
    "                or_matches.append(found)\n",
    "            matches.append(any(or_matches))\n",
    "        #matches = [m for m in matches if m]\n",
    " \n",
    "    if all(matches):\n",
    "        print(data[\"titre\"][i])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.9 ('salle_IA')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "109e681a7444c7d71900bb7b5a5bc18e3244c2a33ec8a0dbbb1ac8e76cc72932"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
